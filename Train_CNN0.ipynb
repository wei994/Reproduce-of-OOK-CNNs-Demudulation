{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0lEPx2ENUukZNnNB2u1f+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"vDfvzfN1fzAB","executionInfo":{"status":"ok","timestamp":1718282186013,"user_tz":-120,"elapsed":27541,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}},"outputId":"a7ed9309-5207-4393-bff3-bd42d896a7af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/UOWC'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["import os\n","import sys\n","import pandas as pd\n","\n","import numpy as np\n","from numpy import exp\n","from numpy import log\n","from numpy import sqrt\n","from numpy import power\n","\n","import scipy\n","from scipy import special\n","\n","import math\n","from math import pi\n","from math import erf\n","from math import erfc\n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","os.getcwd()\n","\n","\n","import tensorflow as tf\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.utils import shuffle\n","from tensorflow import keras\n","from tensorflow.keras.utils import Sequence\n","from tensorflow.keras.models import Sequential\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Input,Softmax\n","from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n","# from tensorflow.keras.callbacks import EarlyStopping, Callback\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/UOWC\")\n","os.getcwd()"]},{"cell_type":"code","source":["class Watertype:\n","  \"\"\"\n","Define the water type and caharacteristics\n","  Attributes:\n","    channel_loss:channel loss diffuse coefficient\n","    Turbulence:\n","  \"\"\"\n","  def __init__(self,channel_loss,Turbulence):\n","    self.channel_loss = channel_loss\n","    self.Turbulence = Turbulence\n","    self.name = \"\""],"metadata":{"id":"KFmQLfJ3f9XM","executionInfo":{"status":"ok","timestamp":1718282191058,"user_tz":-120,"elapsed":283,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Genarate data with constant noise\n","def Simulated_dataset_C(SNR,Z,P,Watertype,Noise='Gauss'):\n","  \"\"\"\n","  This function is used to simulate the input signal to demudulator.\n","  Args:\n","    SNR: signl to noise ratios\n","    Z: channel lenth\n","    P: transmission power\n","    Noise:\n","\n","  Returns:S_1,S_0,Denm_trm1,Denm_trm0,Threshold_optimal\n","   the function returns: numrical signal of bit 1 bit 0 , standard divation of bit 1 noise ,bit 0 noise,optimal Threshold\n","\n","  \"\"\"\n","  ######## Receiver SiPM ::MICROFC−SMTPA−30035\n","  PDE = 0.31 #photo detection efficiency 3mm Sensor 35um - 30035, overvoltage @ 2.5V\n","  lamda = 420e-9 #Wavelength of Blue light\n","  A = 0.0058  #SiPM surface area\n","  G = 3e6#Gain\n","  pap = 0.002 #probability of afterpulsing\n","  dCR = 300 #dark count rate6.6*10^6\n","  Nspad = 4774 #  number of SPAD (microcells)\n","  td = 82e-9  #  tau_d deadtimex\n","  P_AP = 1+pap #  probability of current\n","  pxt = 0.07 # probability of crosstalk\n","  P_XT = 1 + 0.07 #excess noise factor\n","  Fe=1.1  # funtion of the doping profile\n","  RL = 10 #load resistance\n","  F = P_XT\n","  ##### constant\n","  c = 2.25e8 #light speed\n","  hp = 6.626e-34  # Planck's constant h,\n","  K_Z = 1.380649e-23 #Boltzmann constant,\n","  e = 1.6e-19  # Charge of Electron\n","  Res=PDE*lamda*G*e*P_AP*P_XT/(hp*c)  # SiPM responsitivity\n","  #  Laser Diode Parameters\n","  ext=0.33 # extinction ratio\n","  Ptx1 =(1-ext)*P  #[0.25,0.40,0.5] #  power of bit 1 in watt\n","  Ptx0=ext*P    # 0.0008   wpower of bit 0 in watt\n","  ###water characteristics\n","  ce = Watertype.channel_loss ##channel loss diffuse coefficient\n","  Lch=math.exp(-ce*Z) # channel loss\n","  h_L=Lch # channel loss factor\n","\n","  ### Signal dataset infromation\n","  Rb = 10e6#[10,30,60]e6 #transmitt data rate\n","  B=Rb/2 # receiver low-pass filter Bandwidth\n","  lenth=int(1e5)#  length of the signal and noise\n","#####total channel gain\n","  h_gain = h_L #*h_P#*h_T without turbulence without pointing error\n","#######    noise  ######\n","  # sgma2_t_n0  = Ptx0/power(10,SNR/10) #variance of the total signal noise for bit 1\n","  # sgma2_t_n1  = Ptx1/power(10,SNR/10) #variance of the total signal noise for bit 0\n","\n","  T=300 # equivqlent noise temperature [K]\n","  Is_1msm1=Res*Ptx1*RL\n","  Is_0msm0=Res*Ptx0*RL\n","\n","  I_D = e*G*dCR*(1+pap+pxt)\n","  sgma2_T = 4*K_Z*T*B*RL\n","  sgma2_D = 2*e*G*F*B*I_D\n","  sgma2_s_0 = 2*e*G*F*B*Is_0msm0\n","  sgma2_s_1 = 2*e*G*F*B*Is_1msm1\n","  sgma2_t_n0 = sgma2_T +(sgma2_D + sgma2_s_0*h_gain)*(RL**2)\n","  sgma2_t_n1 = sgma2_T +(sgma2_D + sgma2_s_1*h_gain*1)*(RL**2)\n","  # print('sgma2_t_n0',sgma2_t_n0,'sgma2_t_n1',sgma2_t_n0)\n","  #the noise of bit 1 0\n","  if Noise == \"Gauss\":\n","    Noise_0 = np.random.normal(0, np.sqrt(sgma2_s_0*h_gain), lenth)\n","    Noise_1 = np.random.normal(0, np.sqrt(sgma2_s_1*h_gain), lenth)\n","  # the noise is Rayleigh noise\n","  elif Noise == \"Rayleigh\":\n","    Noise_0 = np.random.rayleigh(np.sqrt(sgma2_t_n0), lenth)\n","    Noise_1 = np.random.rayleigh(np.sqrt(sgma2_t_n1), lenth)\n","  elif Noise == \"Uniform\":\n","    Noise_0 = np.random.uniform(-sqrt(sgma2_t_n0),+sqrt(sgma2_s_0),lenth)\n","    Noise_1 = np.random.uniform(-sqrt(sgma2_t_n1),+sqrt(sgma2_s_1),lenth)\n","  else:\n","    print(\"Please choose noise type: Gauss, Rayleigh, Uniform\")\n","    sys.exit()\n","####The optical signal received by the SiPM after underwater propagation\n","  S_R0 = Res*h_gain*Ptx0 + Noise_0 + I_D\n","  S_R1 = Res*h_gain*Ptx1 + Noise_1 + I_D\n","####the input signals to the demodulator corresponding to “0” and “1” bits,\n","  S_0 = RL*S_R0*1e-3 # nurmical simulated current of bit 1\n","  S_1 = RL*S_R1*1e-3 # nurmical simulated current of bit 0\n","#\n","  Denm_trm1 = sqrt(sgma2_t_n1) # standard deviation of bit 1\n","  Denm_trm0 = sqrt(sgma2_t_n0) # standard deviation of bit 0\n","#### Here calculate the optimal threshold\n","  Threshold_optimal = (S_0*sgma2_t_n1  - S_1*sgma2_t_n0 )/(sgma2_t_n1 -sgma2_t_n0 ) + ((S_1**2*sgma2_t_n0 )/(sgma2_t_n1 - sgma2_t_n0 ) + ((S_0*sgma2_t_n1\n","    - S_1*sgma2_t_n0 )/(sgma2_t_n1 -sgma2_t_n0 ))**2 - (S_0**2*sgma2_t_n1 )/(sgma2_t_n1  - sgma2_t_n0 )\n","     - ((sgma2_t_n0*sgma2_t_n1 )/(sgma2_t_n1  - sgma2_t_n0 ))*log(sgma2_t_n0 /sgma2_t_n1 ))**0.5\n","  return S_1,S_0,Denm_trm1,Denm_trm0,Threshold_optimal\n"],"metadata":{"id":"CK19NktNf_Ug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate the dataset base on the channel length and the SNR\n","def Simulated_dataset_V(SNR,Z,P,Watertype,Noise='Gauss'):\n","  \"\"\"\n","  This function is used to simulate the input signal to demudulator.\n","  Args:\n","    SNR: signl to noise ratios\n","    Z: channel lenth\n","    P: transmission power\n","    Noise:\n","\n","  Returns:S_1,S_0,Denm_trm1,Denm_trm0,Threshold_optimal\n","   the function returns: numrical signal of bit 1 bit 0 , standard divation of bit 1 noise ,bit 0 noise,optimal Threshold\n","\n","  \"\"\"\n","  ######## Receiver SiPM ::MICROFC−SMTPA−30035\n","  PDE = 0.31 #photo detection efficiency 3mm Sensor 35um - 30035, overvoltage @ 2.5V\n","  lamda = 420e-9 #Wavelength of Blue light\n","  A = 0.0058  #SiPM surface area\n","  G = 3e6 #Gain\n","  pap = 0.002 #probability of afterpulsing\n","  dCR = 300 #dark count rate6.6*10^6\n","  c = 2.25e8 #light speed\n","  hp = 6.626e-34  # Planck's constant h,\n","  K_Z = 1.380649e-23 #Boltzmann constant,\n","  e = 1.6e-19  # Charge of Electron\n","  Nspad = 4774 #  number of SPAD (microcells)\n","  td = 82e-9  #  tau_d deadtimex\n","  P_AP = 1+pap #  probability of current\n","  pxt = 0.07 # probability of crosstalk\n","  P_XT = 1 + 0.07\n","  Res=PDE*lamda*G*e*P_AP*P_XT/(hp*c)  # what? SiPM responsitivity\n","  Fe=1.1  # funtion of the doping profile\n","  RL = 10 #load resistance\n","  #  Laser Diode Parameters\n","  ext=0.33 # extinction ratio\n","  Ptx1 =(1-ext)*P  #[0.25,0.40,0.5] #  power of bit 1 in watt\n","  Ptx0=ext*P    # 0.0008   wpower of bit 0 in watt\n","  ###water characteristics\n","  ce = Watertype.channel_loss ##channel loss diffuse coefficient\n","  Lch=math.exp(-ce*Z) # channel loss\n","  h_L=Lch # channel loss factor\n","  ### Signal dataset infromation\n","  Rb = 10e6#[10,30,60]e6 #transmitt data rate\n","  lenth=int(1e5)#  length of the signal and noise\n","#####total channel gain\n","  h_gain = h_L #*h_P#*h_T without turbulence without pointing error\n","#######    noise  ######\n","  sgma2_t_n0  = Ptx0/power(10,SNR/10) #variance of the total signal noise for bit 1\n","  sgma2_t_n1  = Ptx1/power(10,SNR/10) #variance of the total signal noise for bit 0\n","  #the noise of bit 1 0\n","  if Noise == \"Gauss\":\n","    Noise_0 = np.random.normal(0, np.sqrt(sgma2_t_n0 ), lenth)\n","    Noise_1 = np.random.normal(0, np.sqrt(sgma2_t_n1 ), lenth)\n","  # the noise is Rayleigh noise\n","  elif Noise == \"Rayleigh\":\n","    Noise_0 = np.random.rayleigh(np.sqrt(sgma2_t_n0 ), lenth)\n","    Noise_1 = np.random.rayleigh(np.sqrt(sgma2_t_n1 ), lenth)\n","  elif Noise == \"Uniform\":\n","    Noise_0 = np.random.uniform(-sqrt(sgma2_t_n0 ),+sqrt(sgma2_t_n0 ),lenth)\n","    Noise_1 = np.random.uniform(-sqrt(sgma2_t_n1 ),+sqrt(sgma2_t_n1 ),lenth)\n","  else:\n","    print(\"Please choose noise type: Gauss, Rayleigh, Uniform\")\n","    sys.exit()\n","####The optical signal received by the SiPM after underwater propagation\n","  S_R0 = Res*h_gain*Ptx0 + Noise_0\n","  S_R1 = Res*h_gain*Ptx1 + Noise_1\n","####the input signals to the demodulator corresponding to “0” and “1” bits,\n","  S_0 = RL*S_R0*1e-6# nurmical simulated current of bit 1\n","  S_1 = RL*S_R1*1e-6# nurmical simulated current of bit 0\n","#\n","  Denm_trm1 = sqrt(sgma2_t_n1 ) # standard deviation of bit 1\n","  Denm_trm0 = sqrt(sgma2_t_n0 ) # standard deviation of bit 0\n","#### Here calculate the optimal threshold\n","  Threshold_optimal = (S_0*sgma2_t_n1  - S_1*sgma2_t_n0 )/(sgma2_t_n1 -sgma2_t_n0 ) + ((S_1**2*sgma2_t_n0 )/(sgma2_t_n1 -sgma2_t_n0 ) + ((S_0*sgma2_t_n1\n","    - S_1*sgma2_t_n0 )/(sgma2_t_n1 -sgma2_t_n0 ))**2 - (S_0**2*sgma2_t_n1 )/(sgma2_t_n1  - sgma2_t_n0 )\n","     - ((sgma2_t_n0 *sgma2_t_n1 )/(sgma2_t_n1  - sgma2_t_n0 ))*log(sgma2_t_n0 /sgma2_t_n1 ))**0.5\n","  return S_1,S_0,Denm_trm1,Denm_trm0,Threshold_optimal"],"metadata":{"id":"8nDrv9PApArD","executionInfo":{"status":"ok","timestamp":1718283874627,"user_tz":-120,"elapsed":240,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Function to create the model\n","def create_model(input_length):\n","    model = Sequential()\n","    model.add(Input(shape=(input_length,)))\n","    model.add(Dense(256))\n","    model.add(BatchNormalization())\n","    model.add(ReLU())\n","    model.add(Dense(96))\n","    model.add(BatchNormalization())\n","    model.add(ReLU())\n","    model.add(Dense(96))\n","    model.add(BatchNormalization())\n","    model.add(ReLU())\n","    model.add(Dense(1, activation='sigmoid'))  # For binary classification\n","    model.compile( optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","    model.summary()\n","    return model"],"metadata":{"id":"hwFnQGD-g8pC","executionInfo":{"status":"ok","timestamp":1718283876544,"user_tz":-120,"elapsed":251,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class DataGenerator(Sequence):\n","    def __init__(self, data, labels, batch_size=512, shuffle=True):\n","        self.data = data\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.indices = np.arange(len(self.data))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        return int(np.floor(len(self.data) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n","        X = self.data[batch_indices]\n","        y = self.labels[batch_indices]\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            np.random.shuffle(self.indices)\n","# Custom callback to print accuracy with more decimal places\n","class CustomCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        accuracy = logs.get('accuracy')\n","        print(f\"Epoch {epoch+1}: Accuracy = {logs['accuracy']:.15f}, Validation Accuracy = {logs['val_accuracy']:.15f}\")\n","\n","class BERCallback(Callback):\n","    def __init__(self):\n","        self.ber_values = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        accuracy = logs.get('accuracy')\n","        ber = 1 - accuracy\n","        self.ber_values.append(ber)\n","        print(f'\\nEpoch {epoch + 1}: BER = {ber:.15f}')\n","# Set up EarlyStopping callback to monitor loss\n","early_stopping = EarlyStopping(monitor='loss', patience=2, verbose=1, mode='min')\n"],"metadata":{"id":"rijZRbiVhLTH","executionInfo":{"status":"ok","timestamp":1718284691124,"user_tz":-120,"elapsed":259,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lXnajBghgBkH","executionInfo":{"status":"ok","timestamp":1718283881257,"user_tz":-120,"elapsed":3,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Define the parameters\n","SNR = 5\n","Z = np.linspace(10,30,10)\n","P = 0.01\n","clearwater = Watertype(0.551,0)\n","# clearwater.channel_loss\n","# early_stopping = EarlyStopping(monitor='loss', patience=2, verbose=1, mode='min')\n","ber_callback = BERCallback()\n","# Learning rate scheduler\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n","\n","Loss = []\n","Accuracy = []\n","for z in Z:\n","\n","  S_1,S_0,Denm_trm1,Denm_trm0,Threshold_optimal = Simulated_dataset_V(SNR,z,P,clearwater,Noise='Gauss')\n","  Label0 = np.zeros(len(S_0))\n","  Label1 = np.ones(len(S_1))\n","  # File path\n","  file_path = 'supervised_dataset.csv'\n","  # Check if the file exists and delete it if it does\n","  if os.path.exists(file_path):\n","      os.remove(file_path)\n","  # Create a DataFrame\n","  data1 = pd.DataFrame({'Feature': S_1, 'Label': Label1})\n","  data0 = pd.DataFrame({'Feature': S_0, 'Label': Label0})\n","  # combine the bit 1 and bit 0 signal\n","  merged_data = pd.concat([data1, data0], ignore_index=True)\n","  # Save the dataset to a CSV file\n","  merged_data.to_csv(file_path, index=False)\n","\n","  data = pd.read_csv(file_path)\n","  shuffled_data = shuffle(data)\n","  # Split the dataset into features (X) and labels (y)\n","  X = shuffled_data[['Feature']].values\n","  Y = shuffled_data['Label'].values\n","  # Split the data into training and testing sets\n","  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","  # Create data generators\n","  batch_size = 512\n","  training_generator = DataGenerator(X_train, Y_train, batch_size=batch_size)\n","  validation_generator = DataGenerator(X_test, Y_test, batch_size=batch_size)\n","  # Adjust input_length to match the number of features\n","  input_length = X_train.shape[1]\n","\n","  # Create the model\n","  CNN0 = create_model(input_length)\n","\n","  # Train the model using the data generator\n","  # history = CNN0.fit(training_generator, validation_data=validation_generator, epochs=30)\n","  history = CNN0.fit(training_generator, validation_data=validation_generator, epochs=30, callbacks=[reduce_lr, ber_callback, early_stopping, CustomCallback()])\n","  # Evaluate the model\n","  loss, accuracy = CNN0.evaluate(validation_generator)\n","\n","  Loss.append(loss)\n","  Accuracy.append(accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNHhsgGtgZaF","executionInfo":{"status":"ok","timestamp":1718293348574,"user_tz":-120,"elapsed":894146,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}},"outputId":"5e1a1091-7fb6-4dd7-83f8-0069f40cee79"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_45\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_180 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_135 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_135 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_181 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_136 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_136 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_182 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_137 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_137 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_183 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","307/312 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9984\n","Epoch 1: BER = 0.001558721065521\n","Epoch 1: Accuracy = 0.998441278934479, Validation Accuracy = 0.873422503471375\n","312/312 [==============================] - 5s 11ms/step - loss: 0.0194 - accuracy: 0.9984 - val_loss: 0.6495 - val_accuracy: 0.8734 - lr: 0.0010\n","Epoch 2/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.4908e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 0.501452326774597\n","312/312 [==============================] - 3s 11ms/step - loss: 2.4826e-04 - accuracy: 1.0000 - val_loss: 1.8913 - val_accuracy: 0.5015 - lr: 0.0010\n","Epoch 3/30\n","312/312 [==============================] - ETA: 0s - loss: 1.0133e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 0.501327097415924\n","312/312 [==============================] - 5s 16ms/step - loss: 1.0133e-04 - accuracy: 1.0000 - val_loss: 6.6042 - val_accuracy: 0.5013 - lr: 0.0010\n","Epoch 4/30\n","312/312 [==============================] - ETA: 0s - loss: 5.4675e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 5.4675e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","312/312 [==============================] - ETA: 0s - loss: 3.3748e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 0.501176893711090\n","312/312 [==============================] - 3s 11ms/step - loss: 3.3748e-05 - accuracy: 1.0000 - val_loss: 1.7349 - val_accuracy: 0.5012 - lr: 0.0010\n","Epoch 6/30\n","312/312 [==============================] - ETA: 0s - loss: 2.2450e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 7s 23ms/step - loss: 2.2450e-05 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.5908e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 1.5896e-05 - accuracy: 1.0000 - val_loss: 1.6546e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.1788e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.1778e-05 - accuracy: 1.0000 - val_loss: 5.9583e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","309/312 [============================>.] - ETA: 0s - loss: 8.8763e-06 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 8.8584e-06 - accuracy: 1.0000 - val_loss: 1.4666e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.6896e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 6.6846e-06 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 11/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.3295e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 5.3258e-06 - accuracy: 1.0000 - val_loss: 5.4206e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 12/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.2539e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 4.2519e-06 - accuracy: 1.0000 - val_loss: 7.7689e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 13/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.7174e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 3.7241e-06 - accuracy: 1.0000 - val_loss: 3.0407e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.6107e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 3.6085e-06 - accuracy: 1.0000 - val_loss: 2.8698e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.2926e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 3.2944e-06 - accuracy: 1.0000 - val_loss: 2.7481e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 16/30\n","307/312 [============================>.] - ETA: 0s - loss: 3.1438e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.1476e-06 - accuracy: 1.0000 - val_loss: 2.5867e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 17/30\n","307/312 [============================>.] - ETA: 0s - loss: 2.9168e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 2.9106e-06 - accuracy: 1.0000 - val_loss: 2.8991e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 18/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.7587e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 2.7596e-06 - accuracy: 1.0000 - val_loss: 2.2732e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.7495e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 2.7507e-06 - accuracy: 1.0000 - val_loss: 2.2173e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.7532e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 2.7514e-06 - accuracy: 1.0000 - val_loss: 2.1769e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 21/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.6441e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.6455e-06 - accuracy: 1.0000 - val_loss: 2.1303e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 22/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.6130e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 2.6121e-06 - accuracy: 1.0000 - val_loss: 2.0691e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 23/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.5750e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 2.5764e-06 - accuracy: 1.0000 - val_loss: 2.0272e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","312/312 [==============================] - ETA: 0s - loss: 2.4806e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 2.4806e-06 - accuracy: 1.0000 - val_loss: 2.0206e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.4794e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 2.4785e-06 - accuracy: 1.0000 - val_loss: 1.9896e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","312/312 [==============================] - ETA: 0s - loss: 2.4952e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 2.4952e-06 - accuracy: 1.0000 - val_loss: 1.9794e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.4452e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 2.4439e-06 - accuracy: 1.0000 - val_loss: 1.9555e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.4109e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 2.4096e-06 - accuracy: 1.0000 - val_loss: 1.9095e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.3511e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 2.3508e-06 - accuracy: 1.0000 - val_loss: 1.8779e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.2321e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 2.2307e-06 - accuracy: 1.0000 - val_loss: 1.8637e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 1s 7ms/step - loss: 1.8637e-06 - accuracy: 1.0000\n","Model: \"sequential_46\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_184 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_138 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_138 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_185 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_139 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_139 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_186 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_140 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_140 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_187 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9984\n","Epoch 1: BER = 0.001633882522583\n","Epoch 1: Accuracy = 0.998366117477417, Validation Accuracy = 0.499899834394455\n","312/312 [==============================] - 6s 12ms/step - loss: 0.0392 - accuracy: 0.9984 - val_loss: 1.7766 - val_accuracy: 0.4999 - lr: 0.0010\n","Epoch 2/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.3011e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 0.500000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 2.2974e-04 - accuracy: 1.0000 - val_loss: 10.7499 - val_accuracy: 0.5000 - lr: 0.0010\n","Epoch 3/30\n","312/312 [==============================] - ETA: 0s - loss: 8.3801e-05 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 0.996694684028625\n","312/312 [==============================] - 5s 17ms/step - loss: 8.3801e-05 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9967 - lr: 0.0010\n","Epoch 4/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.3312e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 0.499799668788910\n","312/312 [==============================] - 4s 12ms/step - loss: 4.3263e-05 - accuracy: 1.0000 - val_loss: 173.2924 - val_accuracy: 0.4998 - lr: 0.0010\n","Epoch 5/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.6389e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 0.499974966049194\n","312/312 [==============================] - 4s 11ms/step - loss: 2.6346e-05 - accuracy: 1.0000 - val_loss: 31.7397 - val_accuracy: 0.5000 - lr: 0.0010\n","Epoch 6/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.6200e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 0.499874800443649\n","312/312 [==============================] - 4s 12ms/step - loss: 1.6193e-05 - accuracy: 1.0000 - val_loss: 361.9958 - val_accuracy: 0.4999 - lr: 0.0010\n","Epoch 7/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.1649e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 0.500225365161896\n","312/312 [==============================] - 5s 15ms/step - loss: 1.1643e-05 - accuracy: 1.0000 - val_loss: 19.6400 - val_accuracy: 0.5002 - lr: 0.0010\n","Epoch 8/30\n","311/312 [============================>.] - ETA: 0s - loss: 8.0939e-06 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 0.499774634838104\n","312/312 [==============================] - 4s 12ms/step - loss: 8.0927e-06 - accuracy: 1.0000 - val_loss: 9.4495 - val_accuracy: 0.4998 - lr: 0.0010\n","Epoch 9/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.8701e-06 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 0.500150263309479\n","312/312 [==============================] - 4s 11ms/step - loss: 6.8610e-06 - accuracy: 1.0000 - val_loss: 18.1437 - val_accuracy: 0.5002 - lr: 2.0000e-04\n","Epoch 10/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.4497e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 0.500100135803223\n","312/312 [==============================] - 5s 17ms/step - loss: 6.4490e-06 - accuracy: 1.0000 - val_loss: 11.2339 - val_accuracy: 0.5001 - lr: 2.0000e-04\n","Epoch 11/30\n","308/312 [============================>.] - ETA: 0s - loss: 6.0215e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 0.500025033950806\n","312/312 [==============================] - 4s 11ms/step - loss: 6.0197e-06 - accuracy: 1.0000 - val_loss: 18.1938 - val_accuracy: 0.5000 - lr: 2.0000e-04\n","Epoch 12/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.5864e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 0.500025033950806\n","312/312 [==============================] - 5s 15ms/step - loss: 5.5820e-06 - accuracy: 1.0000 - val_loss: 9.2619 - val_accuracy: 0.5000 - lr: 2.0000e-04\n","Epoch 13/30\n","312/312 [==============================] - ETA: 0s - loss: 5.2460e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 0.500050067901611\n","312/312 [==============================] - 4s 14ms/step - loss: 5.2460e-06 - accuracy: 1.0000 - val_loss: 17.3652 - val_accuracy: 0.5001 - lr: 2.0000e-04\n","Epoch 14/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.8568e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 4.8518e-06 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 15/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.8660e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 0.499974966049194\n","312/312 [==============================] - 4s 11ms/step - loss: 4.8584e-06 - accuracy: 1.0000 - val_loss: 16.6029 - val_accuracy: 0.5000 - lr: 4.0000e-05\n","Epoch 16/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.6241e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 0.499899834394455\n","312/312 [==============================] - 5s 15ms/step - loss: 4.6181e-06 - accuracy: 1.0000 - val_loss: 2.6675 - val_accuracy: 0.4999 - lr: 4.0000e-05\n","Epoch 17/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.5389e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 0.500100135803223\n","312/312 [==============================] - 4s 11ms/step - loss: 4.5420e-06 - accuracy: 1.0000 - val_loss: 15.2108 - val_accuracy: 0.5001 - lr: 4.0000e-05\n","Epoch 18/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.4543e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 4.4459e-06 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.3121e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 0.500300467014313\n","312/312 [==============================] - 6s 18ms/step - loss: 4.3075e-06 - accuracy: 1.0000 - val_loss: 13.2767 - val_accuracy: 0.5003 - lr: 4.0000e-05\n","Epoch 20/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.2049e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 0.499924868345261\n","312/312 [==============================] - 3s 11ms/step - loss: 4.2043e-06 - accuracy: 1.0000 - val_loss: 18.9247 - val_accuracy: 0.4999 - lr: 4.0000e-05\n","Epoch 21/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.9379e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 0.499749600887299\n","312/312 [==============================] - 4s 11ms/step - loss: 3.9342e-06 - accuracy: 1.0000 - val_loss: 12.6517 - val_accuracy: 0.4997 - lr: 4.0000e-05\n","Epoch 22/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.8167e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 0.500275433063507\n","312/312 [==============================] - 4s 13ms/step - loss: 3.8143e-06 - accuracy: 1.0000 - val_loss: 15.8756 - val_accuracy: 0.5003 - lr: 4.0000e-05\n","Epoch 23/30\n","308/312 [============================>.] - ETA: 0s - loss: 3.5168e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 0.499849766492844\n","312/312 [==============================] - 4s 14ms/step - loss: 3.5167e-06 - accuracy: 1.0000 - val_loss: 9.3150 - val_accuracy: 0.4998 - lr: 4.0000e-05\n","Epoch 24/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.4904e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 3.5010e-06 - accuracy: 1.0000 - val_loss: 4.4799e-04 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.4136e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 3.4124e-06 - accuracy: 1.0000 - val_loss: 9.5765e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","312/312 [==============================] - ETA: 0s - loss: 3.3962e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 0.839418053627014\n","312/312 [==============================] - 5s 16ms/step - loss: 3.3962e-06 - accuracy: 1.0000 - val_loss: 0.3048 - val_accuracy: 0.8394 - lr: 1.0000e-05\n","Epoch 27/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.2299e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 0.995918452739716\n","312/312 [==============================] - 4s 12ms/step - loss: 3.2258e-06 - accuracy: 1.0000 - val_loss: 0.0866 - val_accuracy: 0.9959 - lr: 1.0000e-05\n","Epoch 28/30\n","312/312 [==============================] - ETA: 0s - loss: 3.2122e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 0.500025033950806\n","312/312 [==============================] - 4s 11ms/step - loss: 3.2122e-06 - accuracy: 1.0000 - val_loss: 1.5915 - val_accuracy: 0.5000 - lr: 1.0000e-05\n","Epoch 29/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.1114e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 0.630734145641327\n","312/312 [==============================] - 4s 11ms/step - loss: 3.1087e-06 - accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.6307 - lr: 1.0000e-05\n","Epoch 30/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.0244e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 3.0227e-06 - accuracy: 1.0000 - val_loss: 2.4961e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 4ms/step - loss: 2.4958e-06 - accuracy: 1.0000\n","Model: \"sequential_47\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_188 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_141 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_141 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_189 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_142 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_142 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_190 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_143 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_143 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_191 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9984\n","Epoch 1: BER = 0.001590073108673\n","Epoch 1: Accuracy = 0.998409926891327, Validation Accuracy = 0.502453923225403\n","312/312 [==============================] - 29s 13ms/step - loss: 0.0747 - accuracy: 0.9984 - val_loss: 3.7733 - val_accuracy: 0.5025 - lr: 0.0010\n","Epoch 2/30\n","312/312 [==============================] - ETA: 0s - loss: 2.5116e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 0.502378821372986\n","312/312 [==============================] - 6s 20ms/step - loss: 2.5116e-04 - accuracy: 1.0000 - val_loss: 16.7235 - val_accuracy: 0.5024 - lr: 0.0010\n","Epoch 3/30\n","312/312 [==============================] - ETA: 0s - loss: 8.3514e-05 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 0.502278625965118\n","312/312 [==============================] - 4s 12ms/step - loss: 8.3514e-05 - accuracy: 1.0000 - val_loss: 17.2533 - val_accuracy: 0.5023 - lr: 0.0010\n","Epoch 4/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.1242e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 0.497370779514313\n","312/312 [==============================] - 4s 12ms/step - loss: 4.1085e-05 - accuracy: 1.0000 - val_loss: 136.9542 - val_accuracy: 0.4974 - lr: 0.0010\n","Epoch 5/30\n","312/312 [==============================] - ETA: 0s - loss: 2.3697e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 0.502503991127014\n","312/312 [==============================] - 5s 16ms/step - loss: 2.3697e-05 - accuracy: 1.0000 - val_loss: 75.0903 - val_accuracy: 0.5025 - lr: 0.0010\n","Epoch 6/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.5346e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 0.497495979070663\n","312/312 [==============================] - 5s 15ms/step - loss: 1.5350e-05 - accuracy: 1.0000 - val_loss: 10.9980 - val_accuracy: 0.4975 - lr: 0.0010\n","Epoch 7/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.1999e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 0.502253592014313\n","312/312 [==============================] - 4s 12ms/step - loss: 1.1990e-05 - accuracy: 1.0000 - val_loss: 21.6840 - val_accuracy: 0.5023 - lr: 2.0000e-04\n","Epoch 8/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.1128e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 0.846329152584076\n","312/312 [==============================] - 4s 13ms/step - loss: 1.1127e-05 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.8463 - lr: 2.0000e-04\n","Epoch 9/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.0077e-05 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 0.497596144676208\n","312/312 [==============================] - 5s 17ms/step - loss: 1.0123e-05 - accuracy: 1.0000 - val_loss: 9.1596 - val_accuracy: 0.4976 - lr: 2.0000e-04\n","Epoch 10/30\n","312/312 [==============================] - ETA: 0s - loss: 9.1580e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 9.1580e-06 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 11/30\n","310/312 [============================>.] - ETA: 0s - loss: 8.2914e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 8.2952e-06 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","312/312 [==============================] - ETA: 0s - loss: 7.4071e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 0.502353787422180\n","312/312 [==============================] - 4s 12ms/step - loss: 7.4071e-06 - accuracy: 1.0000 - val_loss: 67.3923 - val_accuracy: 0.5024 - lr: 2.0000e-04\n","Epoch 13/30\n","312/312 [==============================] - ETA: 0s - loss: 6.4969e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 0.497546076774597\n","312/312 [==============================] - 4s 12ms/step - loss: 6.4969e-06 - accuracy: 1.0000 - val_loss: 3.3610 - val_accuracy: 0.4975 - lr: 2.0000e-04\n","Epoch 14/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.8672e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 0.497621208429337\n","312/312 [==============================] - 6s 18ms/step - loss: 5.8588e-06 - accuracy: 1.0000 - val_loss: 110.1816 - val_accuracy: 0.4976 - lr: 2.0000e-04\n","Epoch 15/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.9250e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 0.497621208429337\n","312/312 [==============================] - 4s 13ms/step - loss: 4.9446e-06 - accuracy: 1.0000 - val_loss: 4.2296 - val_accuracy: 0.4976 - lr: 2.0000e-04\n","Epoch 16/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.5631e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 0.502503991127014\n","312/312 [==============================] - 4s 12ms/step - loss: 4.5650e-06 - accuracy: 1.0000 - val_loss: 64.8590 - val_accuracy: 0.5025 - lr: 2.0000e-04\n","Epoch 17/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.2097e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 14ms/step - loss: 4.2226e-06 - accuracy: 1.0000 - val_loss: 4.5665e-04 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","308/312 [============================>.] - ETA: 0s - loss: 3.8978e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 0.502478957176208\n","312/312 [==============================] - 5s 16ms/step - loss: 3.8999e-06 - accuracy: 1.0000 - val_loss: 2.7569 - val_accuracy: 0.5025 - lr: 4.0000e-05\n","Epoch 19/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.7980e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 0.989708542823792\n","312/312 [==============================] - 4s 12ms/step - loss: 3.7976e-06 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9897 - lr: 4.0000e-05\n","Epoch 20/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.9024e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 0.502453923225403\n","312/312 [==============================] - 4s 12ms/step - loss: 3.8966e-06 - accuracy: 1.0000 - val_loss: 19.9385 - val_accuracy: 0.5025 - lr: 4.0000e-05\n","Epoch 21/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.5113e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 0.497696310281754\n","312/312 [==============================] - 6s 19ms/step - loss: 3.5166e-06 - accuracy: 1.0000 - val_loss: 60.6401 - val_accuracy: 0.4977 - lr: 4.0000e-05\n","Epoch 22/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.3751e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 3.3725e-06 - accuracy: 1.0000 - val_loss: 9.7060e-04 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 23/30\n","312/312 [==============================] - ETA: 0s - loss: 3.2219e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 0.502704322338104\n","312/312 [==============================] - 5s 16ms/step - loss: 3.2219e-06 - accuracy: 1.0000 - val_loss: 8.9824 - val_accuracy: 0.5027 - lr: 1.0000e-05\n","Epoch 24/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.4455e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 0.981820940971375\n","312/312 [==============================] - 5s 16ms/step - loss: 3.4471e-06 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9818 - lr: 1.0000e-05\n","Epoch 25/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.2727e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 0.957782447338104\n","312/312 [==============================] - 4s 13ms/step - loss: 3.2772e-06 - accuracy: 1.0000 - val_loss: 0.1154 - val_accuracy: 0.9578 - lr: 1.0000e-05\n","Epoch 25: early stopping\n","78/78 [==============================] - 0s 4ms/step - loss: 0.1155 - accuracy: 0.9577\n","Model: \"sequential_48\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_192 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_144 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_144 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_193 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_145 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_145 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_194 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_146 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_146 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_195 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","312/312 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9768\n","Epoch 1: BER = 0.023205876350403\n","Epoch 1: Accuracy = 0.976794123649597, Validation Accuracy = 0.496719747781754\n","312/312 [==============================] - 8s 20ms/step - loss: 0.1529 - accuracy: 0.9768 - val_loss: 19.5250 - val_accuracy: 0.4967 - lr: 0.0010\n","Epoch 2/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9779\n","Epoch 2: BER = 0.022110402584076\n","Epoch 2: Accuracy = 0.977889597415924, Validation Accuracy = 0.496544480323792\n","312/312 [==============================] - 4s 12ms/step - loss: 0.0619 - accuracy: 0.9779 - val_loss: 2343.8145 - val_accuracy: 0.4965 - lr: 0.0010\n","Epoch 3/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9774\n","Epoch 3: BER = 0.022586166858673\n","Epoch 3: Accuracy = 0.977413833141327, Validation Accuracy = 0.496594548225403\n","312/312 [==============================] - 4s 12ms/step - loss: 0.0615 - accuracy: 0.9774 - val_loss: 843.6939 - val_accuracy: 0.4966 - lr: 0.0010\n","Epoch 4/30\n","312/312 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9778\n","Epoch 4: BER = 0.022235572338104\n","Epoch 4: Accuracy = 0.977764427661896, Validation Accuracy = 0.496444314718246\n","312/312 [==============================] - 5s 16ms/step - loss: 0.0614 - accuracy: 0.9778 - val_loss: 9528.8018 - val_accuracy: 0.4964 - lr: 0.0010\n","Epoch 5/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9776\n","Epoch 5: BER = 0.022385835647583\n","Epoch 5: Accuracy = 0.977614164352417, Validation Accuracy = 0.496594548225403\n","312/312 [==============================] - 5s 16ms/step - loss: 0.0612 - accuracy: 0.9776 - val_loss: 8786.9512 - val_accuracy: 0.4966 - lr: 0.0010\n","Epoch 6/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9774\n","Epoch 6: BER = 0.022629976272583\n","Epoch 6: Accuracy = 0.977370023727417, Validation Accuracy = 0.496819913387299\n","312/312 [==============================] - 4s 12ms/step - loss: 0.0615 - accuracy: 0.9774 - val_loss: 1763.6737 - val_accuracy: 0.4968 - lr: 0.0010\n","Epoch 7/30\n","307/312 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9777\n","Epoch 7: BER = 0.022323191165924\n","Epoch 7: Accuracy = 0.977676808834076, Validation Accuracy = 0.496669679880142\n","312/312 [==============================] - 4s 12ms/step - loss: 0.0613 - accuracy: 0.9777 - val_loss: 388.3842 - val_accuracy: 0.4967 - lr: 2.0000e-04\n","Epoch 7: early stopping\n","78/78 [==============================] - 0s 6ms/step - loss: 388.3455 - accuracy: 0.4967\n","Model: \"sequential_49\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_196 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_147 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_147 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_197 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_148 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_148 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_198 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_149 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_149 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_199 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.5592 - accuracy: 0.7279\n","Epoch 1: BER = 0.272260606288910\n","Epoch 1: Accuracy = 0.727739393711090, Validation Accuracy = 0.500525832176208\n","312/312 [==============================] - 7s 14ms/step - loss: 0.5593 - accuracy: 0.7277 - val_loss: 23.2512 - val_accuracy: 0.5005 - lr: 0.0010\n","Epoch 2/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.5357 - accuracy: 0.7298\n","Epoch 2: BER = 0.270388901233673\n","Epoch 2: Accuracy = 0.729611098766327, Validation Accuracy = 0.499649435281754\n","312/312 [==============================] - 4s 12ms/step - loss: 0.5360 - accuracy: 0.7296 - val_loss: 114.5122 - val_accuracy: 0.4996 - lr: 0.0010\n","Epoch 3/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.5358 - accuracy: 0.7292\n","Epoch 3: BER = 0.270764470100403\n","Epoch 3: Accuracy = 0.729235529899597, Validation Accuracy = 0.500475764274597\n","312/312 [==============================] - 5s 16ms/step - loss: 0.5358 - accuracy: 0.7292 - val_loss: 4528.2466 - val_accuracy: 0.5005 - lr: 0.0010\n","Epoch 4/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.5353 - accuracy: 0.7292\n","Epoch 4: BER = 0.270870864391327\n","Epoch 4: Accuracy = 0.729129135608673, Validation Accuracy = 0.500375628471375\n","312/312 [==============================] - 5s 16ms/step - loss: 0.5354 - accuracy: 0.7291 - val_loss: 455.6441 - val_accuracy: 0.5004 - lr: 0.0010\n","Epoch 5/30\n","312/312 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.7293\n","Epoch 5: BER = 0.270726919174194\n","Epoch 5: Accuracy = 0.729273080825806, Validation Accuracy = 0.500475764274597\n","312/312 [==============================] - 4s 12ms/step - loss: 0.5350 - accuracy: 0.7293 - val_loss: 395.0302 - val_accuracy: 0.5005 - lr: 0.0010\n","Epoch 6/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.5349 - accuracy: 0.7293\n","Epoch 6: BER = 0.270701885223389\n","Epoch 6: Accuracy = 0.729298114776611, Validation Accuracy = 0.499424070119858\n","312/312 [==============================] - 4s 12ms/step - loss: 0.5349 - accuracy: 0.7293 - val_loss: 151.8991 - val_accuracy: 0.4994 - lr: 0.0010\n","Epoch 7/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.5345 - accuracy: 0.7293\n","Epoch 7: BER = 0.270708143711090\n","Epoch 7: Accuracy = 0.729291856288910, Validation Accuracy = 0.499674469232559\n","312/312 [==============================] - 6s 19ms/step - loss: 0.5346 - accuracy: 0.7293 - val_loss: 30.4189 - val_accuracy: 0.4997 - lr: 2.0000e-04\n","Epoch 8/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7292\n","Epoch 8: BER = 0.270739436149597\n","Epoch 8: Accuracy = 0.729260563850403, Validation Accuracy = 0.499599367380142\n","312/312 [==============================] - 4s 12ms/step - loss: 0.5345 - accuracy: 0.7293 - val_loss: 13.3136 - val_accuracy: 0.4996 - lr: 2.0000e-04\n","Epoch 9/30\n","312/312 [==============================] - ETA: 0s - loss: 0.5346 - accuracy: 0.7292\n","Epoch 9: BER = 0.270770728588104\n","Epoch 9: Accuracy = 0.729229271411896, Validation Accuracy = 0.499449104070663\n","312/312 [==============================] - 5s 17ms/step - loss: 0.5346 - accuracy: 0.7292 - val_loss: 16.5066 - val_accuracy: 0.4994 - lr: 2.0000e-04\n","Epoch 10/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7291\n","Epoch 10: BER = 0.270902216434479\n","Epoch 10: Accuracy = 0.729097783565521, Validation Accuracy = 0.499499201774597\n","312/312 [==============================] - 5s 15ms/step - loss: 0.5347 - accuracy: 0.7291 - val_loss: 0.7349 - val_accuracy: 0.4995 - lr: 2.0000e-04\n","Epoch 10: early stopping\n","78/78 [==============================] - 0s 4ms/step - loss: 0.7349 - accuracy: 0.4994\n","Model: \"sequential_50\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_200 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_150 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_150 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_201 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_151 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_151 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_202 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_152 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_152 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_203 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6602 - accuracy: 0.5932\n","Epoch 1: BER = 0.406763315200806\n","Epoch 1: Accuracy = 0.593236684799194, Validation Accuracy = 0.499924868345261\n","312/312 [==============================] - 6s 13ms/step - loss: 0.6602 - accuracy: 0.5932 - val_loss: 3.7744 - val_accuracy: 0.4999 - lr: 0.0010\n","Epoch 2/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6534 - accuracy: 0.6026\n","Epoch 2: BER = 0.397310674190521\n","Epoch 2: Accuracy = 0.602689325809479, Validation Accuracy = 0.499524235725403\n","312/312 [==============================] - 6s 18ms/step - loss: 0.6533 - accuracy: 0.6027 - val_loss: 73.7331 - val_accuracy: 0.4995 - lr: 0.0010\n","Epoch 3/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6528 - accuracy: 0.6023\n","Epoch 3: BER = 0.397673785686493\n","Epoch 3: Accuracy = 0.602326214313507, Validation Accuracy = 0.499724566936493\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6527 - accuracy: 0.6023 - val_loss: 1543.4077 - val_accuracy: 0.4997 - lr: 0.0010\n","Epoch 4/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6526 - accuracy: 0.6020\n","Epoch 4: BER = 0.397823989391327\n","Epoch 4: Accuracy = 0.602176010608673, Validation Accuracy = 0.499874800443649\n","312/312 [==============================] - 5s 18ms/step - loss: 0.6525 - accuracy: 0.6022 - val_loss: 1180.9880 - val_accuracy: 0.4999 - lr: 0.0010\n","Epoch 5/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6525 - accuracy: 0.6029\n","Epoch 5: BER = 0.397072792053223\n","Epoch 5: Accuracy = 0.602927207946777, Validation Accuracy = 0.499849766492844\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6524 - accuracy: 0.6029 - val_loss: 1218.7201 - val_accuracy: 0.4998 - lr: 0.0010\n","Epoch 6/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6522 - accuracy: 0.6026\n","Epoch 6: BER = 0.397216796875000\n","Epoch 6: Accuracy = 0.602783203125000, Validation Accuracy = 0.499849766492844\n","312/312 [==============================] - 5s 16ms/step - loss: 0.6521 - accuracy: 0.6028 - val_loss: 412.4310 - val_accuracy: 0.4998 - lr: 0.0010\n","Epoch 7/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.6518 - accuracy: 0.6034\n","Epoch 7: BER = 0.396647155284882\n","Epoch 7: Accuracy = 0.603352844715118, Validation Accuracy = 0.499724566936493\n","312/312 [==============================] - 5s 15ms/step - loss: 0.6518 - accuracy: 0.6034 - val_loss: 119.4847 - val_accuracy: 0.4997 - lr: 2.0000e-04\n","Epoch 8/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.6518 - accuracy: 0.6032\n","Epoch 8: BER = 0.396809875965118\n","Epoch 8: Accuracy = 0.603190124034882, Validation Accuracy = 0.499799668788910\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6518 - accuracy: 0.6032 - val_loss: 727.1104 - val_accuracy: 0.4998 - lr: 2.0000e-04\n","Epoch 9/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6518 - accuracy: 0.6032\n","Epoch 9: BER = 0.396985173225403\n","Epoch 9: Accuracy = 0.603014826774597, Validation Accuracy = 0.499724566936493\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6518 - accuracy: 0.6030 - val_loss: 381.8055 - val_accuracy: 0.4997 - lr: 2.0000e-04\n","Epoch 10/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6518 - accuracy: 0.6027\n","Epoch 10: BER = 0.397279381752014\n","Epoch 10: Accuracy = 0.602720618247986, Validation Accuracy = 0.499799668788910\n","312/312 [==============================] - 5s 17ms/step - loss: 0.6518 - accuracy: 0.6027 - val_loss: 1262.2140 - val_accuracy: 0.4998 - lr: 2.0000e-04\n","Epoch 11/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.6519 - accuracy: 0.6027\n","Epoch 11: BER = 0.397223055362701\n","Epoch 11: Accuracy = 0.602776944637299, Validation Accuracy = 0.499774634838104\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6519 - accuracy: 0.6028 - val_loss: 264.9694 - val_accuracy: 0.4998 - lr: 2.0000e-04\n","Epoch 12/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6518 - accuracy: 0.6031\n","Epoch 12: BER = 0.396910071372986\n","Epoch 12: Accuracy = 0.603089928627014, Validation Accuracy = 0.499749600887299\n","312/312 [==============================] - 5s 17ms/step - loss: 0.6518 - accuracy: 0.6031 - val_loss: 391.3691 - val_accuracy: 0.4997 - lr: 4.0000e-05\n","Epoch 12: early stopping\n","78/78 [==============================] - 1s 6ms/step - loss: 391.4670 - accuracy: 0.4996\n","Model: \"sequential_51\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_204 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_153 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_153 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_205 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_154 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_154 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_206 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_155 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_155 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_207 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6705 - accuracy: 0.5723\n","Epoch 1: BER = 0.427778184413910\n","Epoch 1: Accuracy = 0.572221815586090, Validation Accuracy = 0.496118783950806\n","312/312 [==============================] - 6s 13ms/step - loss: 0.6706 - accuracy: 0.5722 - val_loss: 6.7568 - val_accuracy: 0.4961 - lr: 0.0010\n","Epoch 2/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.6635 - accuracy: 0.5871\n","Epoch 2: BER = 0.412879347801208\n","Epoch 2: Accuracy = 0.587120652198792, Validation Accuracy = 0.496143817901611\n","312/312 [==============================] - 4s 13ms/step - loss: 0.6634 - accuracy: 0.5871 - val_loss: 1069.2659 - val_accuracy: 0.4961 - lr: 0.0010\n","Epoch 3/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6630 - accuracy: 0.5869\n","Epoch 3: BER = 0.413129746913910\n","Epoch 3: Accuracy = 0.586870253086090, Validation Accuracy = 0.496068716049194\n","312/312 [==============================] - 5s 16ms/step - loss: 0.6631 - accuracy: 0.5869 - val_loss: 6750.7490 - val_accuracy: 0.4961 - lr: 0.0010\n","Epoch 4/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6629 - accuracy: 0.5876\n","Epoch 4: BER = 0.412422358989716\n","Epoch 4: Accuracy = 0.587577641010284, Validation Accuracy = 0.495943516492844\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6629 - accuracy: 0.5876 - val_loss: 1137.6986 - val_accuracy: 0.4959 - lr: 0.0010\n","Epoch 5/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6626 - accuracy: 0.5877\n","Epoch 5: BER = 0.412215769290924\n","Epoch 5: Accuracy = 0.587784230709076, Validation Accuracy = 0.495968550443649\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6626 - accuracy: 0.5878 - val_loss: 5470.4194 - val_accuracy: 0.4960 - lr: 0.0010\n","Epoch 6/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6626 - accuracy: 0.5867\n","Epoch 6: BER = 0.413254976272583\n","Epoch 6: Accuracy = 0.586745023727417, Validation Accuracy = 0.496018618345261\n","312/312 [==============================] - 5s 16ms/step - loss: 0.6626 - accuracy: 0.5867 - val_loss: 2736.0977 - val_accuracy: 0.4960 - lr: 0.0010\n","Epoch 7/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.5871\n","Epoch 7: BER = 0.412923157215118\n","Epoch 7: Accuracy = 0.587076842784882, Validation Accuracy = 0.496068716049194\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6623 - accuracy: 0.5871 - val_loss: 765.2964 - val_accuracy: 0.4961 - lr: 2.0000e-04\n","Epoch 8/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.5873\n","Epoch 8: BER = 0.412635207176208\n","Epoch 8: Accuracy = 0.587364792823792, Validation Accuracy = 0.496018618345261\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6623 - accuracy: 0.5874 - val_loss: 641.4293 - val_accuracy: 0.4960 - lr: 2.0000e-04\n","Epoch 9/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.5876\n","Epoch 9: BER = 0.412384808063507\n","Epoch 9: Accuracy = 0.587615191936493, Validation Accuracy = 0.495993584394455\n","312/312 [==============================] - 5s 17ms/step - loss: 0.6623 - accuracy: 0.5876 - val_loss: 755.0076 - val_accuracy: 0.4960 - lr: 2.0000e-04\n","Epoch 10/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.5877\n","Epoch 10: BER = 0.412491261959076\n","Epoch 10: Accuracy = 0.587508738040924, Validation Accuracy = 0.495968550443649\n","312/312 [==============================] - 5s 14ms/step - loss: 0.6624 - accuracy: 0.5875 - val_loss: 1055.9619 - val_accuracy: 0.4960 - lr: 2.0000e-04\n","Epoch 11/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.5879\n","Epoch 11: BER = 0.412203252315521\n","Epoch 11: Accuracy = 0.587796747684479, Validation Accuracy = 0.495843350887299\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6623 - accuracy: 0.5878 - val_loss: 471.2833 - val_accuracy: 0.4958 - lr: 2.0000e-04\n","Epoch 12/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.6621 - accuracy: 0.5879\n","Epoch 12: BER = 0.412516295909882\n","Epoch 12: Accuracy = 0.587483704090118, Validation Accuracy = 0.496068716049194\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6622 - accuracy: 0.5875 - val_loss: 130.0806 - val_accuracy: 0.4961 - lr: 4.0000e-05\n","Epoch 13/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.5874\n","Epoch 13: BER = 0.412566363811493\n","Epoch 13: Accuracy = 0.587433636188507, Validation Accuracy = 0.495993584394455\n","312/312 [==============================] - 6s 18ms/step - loss: 0.6623 - accuracy: 0.5874 - val_loss: 109.5284 - val_accuracy: 0.4960 - lr: 4.0000e-05\n","Epoch 14/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.5875\n","Epoch 14: BER = 0.412391066551208\n","Epoch 14: Accuracy = 0.587608933448792, Validation Accuracy = 0.495943516492844\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6624 - accuracy: 0.5876 - val_loss: 47.0900 - val_accuracy: 0.4959 - lr: 4.0000e-05\n","Epoch 14: early stopping\n","78/78 [==============================] - 0s 4ms/step - loss: 47.0712 - accuracy: 0.4961\n","Model: \"sequential_52\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_208 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_156 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_156 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_209 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_157 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_157 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_210 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_158 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_158 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_211 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6739 - accuracy: 0.5704\n","Epoch 1: BER = 0.429662466049194\n","Epoch 1: Accuracy = 0.570337533950806, Validation Accuracy = 0.497395843267441\n","312/312 [==============================] - 8s 17ms/step - loss: 0.6739 - accuracy: 0.5703 - val_loss: 12.7027 - val_accuracy: 0.4974 - lr: 0.0010\n","Epoch 2/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6656 - accuracy: 0.5844\n","Epoch 2: BER = 0.415614962577820\n","Epoch 2: Accuracy = 0.584385037422180, Validation Accuracy = 0.497470945119858\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6656 - accuracy: 0.5844 - val_loss: 197.3124 - val_accuracy: 0.4975 - lr: 0.0010\n","Epoch 3/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6652 - accuracy: 0.5849\n","Epoch 3: BER = 0.415107905864716\n","Epoch 3: Accuracy = 0.584892094135284, Validation Accuracy = 0.497370779514313\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6652 - accuracy: 0.5849 - val_loss: 1263.8165 - val_accuracy: 0.4974 - lr: 0.0010\n","Epoch 4/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6648 - accuracy: 0.5857\n","Epoch 4: BER = 0.414269089698792\n","Epoch 4: Accuracy = 0.585730910301208, Validation Accuracy = 0.497245579957962\n","312/312 [==============================] - 5s 17ms/step - loss: 0.6648 - accuracy: 0.5857 - val_loss: 2019.7832 - val_accuracy: 0.4972 - lr: 0.0010\n","Epoch 5/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.5851\n","Epoch 5: BER = 0.414932668209076\n","Epoch 5: Accuracy = 0.585067331790924, Validation Accuracy = 0.497470945119858\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6647 - accuracy: 0.5851 - val_loss: 416.3914 - val_accuracy: 0.4975 - lr: 0.0010\n","Epoch 6/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.5850\n","Epoch 6: BER = 0.414907574653625\n","Epoch 6: Accuracy = 0.585092425346375, Validation Accuracy = 0.497220546007156\n","312/312 [==============================] - 6s 18ms/step - loss: 0.6647 - accuracy: 0.5851 - val_loss: 1955.3081 - val_accuracy: 0.4972 - lr: 0.0010\n","Epoch 7/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6645 - accuracy: 0.5851\n","Epoch 7: BER = 0.414963960647583\n","Epoch 7: Accuracy = 0.585036039352417, Validation Accuracy = 0.497170478105545\n","312/312 [==============================] - 4s 14ms/step - loss: 0.6646 - accuracy: 0.5850 - val_loss: 187.2614 - val_accuracy: 0.4972 - lr: 2.0000e-04\n","Epoch 8/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6644 - accuracy: 0.5852\n","Epoch 8: BER = 0.414763629436493\n","Epoch 8: Accuracy = 0.585236370563507, Validation Accuracy = 0.497195512056351\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6644 - accuracy: 0.5852 - val_loss: 779.1878 - val_accuracy: 0.4972 - lr: 2.0000e-04\n","Epoch 9/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6645 - accuracy: 0.5844\n","Epoch 9: BER = 0.415489792823792\n","Epoch 9: Accuracy = 0.584510207176208, Validation Accuracy = 0.497195512056351\n","312/312 [==============================] - 5s 15ms/step - loss: 0.6645 - accuracy: 0.5845 - val_loss: 34.6028 - val_accuracy: 0.4972 - lr: 2.0000e-04\n","Epoch 10/30\n","307/312 [============================>.] - ETA: 0s - loss: 0.6645 - accuracy: 0.5847\n","Epoch 10: BER = 0.415233135223389\n","Epoch 10: Accuracy = 0.584766864776611, Validation Accuracy = 0.497295677661896\n","312/312 [==============================] - 5s 15ms/step - loss: 0.6645 - accuracy: 0.5848 - val_loss: 483.4309 - val_accuracy: 0.4973 - lr: 2.0000e-04\n","Epoch 10: early stopping\n","78/78 [==============================] - 0s 4ms/step - loss: 483.3346 - accuracy: 0.4974\n","Model: \"sequential_53\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_212 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_159 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_159 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_213 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_160 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_160 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_214 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_161 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_161 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_215 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6728 - accuracy: 0.5668\n","Epoch 1: BER = 0.433011591434479\n","Epoch 1: Accuracy = 0.566988408565521, Validation Accuracy = 0.498222142457962\n","312/312 [==============================] - 6s 13ms/step - loss: 0.6727 - accuracy: 0.5670 - val_loss: 11.5218 - val_accuracy: 0.4982 - lr: 0.0010\n","Epoch 2/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6655 - accuracy: 0.5842\n","Epoch 2: BER = 0.415821552276611\n","Epoch 2: Accuracy = 0.584178447723389, Validation Accuracy = 0.498046875000000\n","312/312 [==============================] - 6s 18ms/step - loss: 0.6654 - accuracy: 0.5842 - val_loss: 139.1988 - val_accuracy: 0.4980 - lr: 0.0010\n","Epoch 3/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6652 - accuracy: 0.5844\n","Epoch 3: BER = 0.415596187114716\n","Epoch 3: Accuracy = 0.584403812885284, Validation Accuracy = 0.498046875000000\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6652 - accuracy: 0.5844 - val_loss: 1341.2910 - val_accuracy: 0.4980 - lr: 0.0010\n","Epoch 4/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6651 - accuracy: 0.5844\n","Epoch 4: BER = 0.415633738040924\n","Epoch 4: Accuracy = 0.584366261959076, Validation Accuracy = 0.498172074556351\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6651 - accuracy: 0.5844 - val_loss: 2425.9851 - val_accuracy: 0.4982 - lr: 0.0010\n","Epoch 5/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6650 - accuracy: 0.5839\n","Epoch 5: BER = 0.416059434413910\n","Epoch 5: Accuracy = 0.583940565586090, Validation Accuracy = 0.498197108507156\n","312/312 [==============================] - 5s 16ms/step - loss: 0.6650 - accuracy: 0.5839 - val_loss: 366.9609 - val_accuracy: 0.4982 - lr: 0.0010\n","Epoch 6/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6649 - accuracy: 0.5843\n","Epoch 6: BER = 0.415596187114716\n","Epoch 6: Accuracy = 0.584403812885284, Validation Accuracy = 0.498147040605545\n","312/312 [==============================] - 5s 16ms/step - loss: 0.6649 - accuracy: 0.5844 - val_loss: 1240.8040 - val_accuracy: 0.4981 - lr: 0.0010\n","Epoch 7/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.5846\n","Epoch 7: BER = 0.415571153163910\n","Epoch 7: Accuracy = 0.584428846836090, Validation Accuracy = 0.498122006654739\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6647 - accuracy: 0.5844 - val_loss: 159.5091 - val_accuracy: 0.4981 - lr: 2.0000e-04\n","Epoch 8/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.5844\n","Epoch 8: BER = 0.415533602237701\n","Epoch 8: Accuracy = 0.584466397762299, Validation Accuracy = 0.498021841049194\n","312/312 [==============================] - 4s 13ms/step - loss: 0.6646 - accuracy: 0.5845 - val_loss: 1345.6525 - val_accuracy: 0.4980 - lr: 2.0000e-04\n","Epoch 9/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6647 - accuracy: 0.5843\n","Epoch 9: BER = 0.415677607059479\n","Epoch 9: Accuracy = 0.584322392940521, Validation Accuracy = 0.498071908950806\n","312/312 [==============================] - 6s 18ms/step - loss: 0.6647 - accuracy: 0.5843 - val_loss: 764.0030 - val_accuracy: 0.4981 - lr: 2.0000e-04\n","Epoch 10/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.5846\n","Epoch 10: BER = 0.415414690971375\n","Epoch 10: Accuracy = 0.584585309028625, Validation Accuracy = 0.498172074556351\n","312/312 [==============================] - 4s 13ms/step - loss: 0.6646 - accuracy: 0.5846 - val_loss: 1205.7469 - val_accuracy: 0.4982 - lr: 2.0000e-04\n","Epoch 11/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.5844\n","Epoch 11: BER = 0.415621221065521\n","Epoch 11: Accuracy = 0.584378778934479, Validation Accuracy = 0.497996807098389\n","312/312 [==============================] - 6s 18ms/step - loss: 0.6646 - accuracy: 0.5844 - val_loss: 310.5871 - val_accuracy: 0.4980 - lr: 2.0000e-04\n","Epoch 12/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6646 - accuracy: 0.5842\n","Epoch 12: BER = 0.415621221065521\n","Epoch 12: Accuracy = 0.584378778934479, Validation Accuracy = 0.497996807098389\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6645 - accuracy: 0.5844 - val_loss: 247.6509 - val_accuracy: 0.4980 - lr: 4.0000e-05\n","Epoch 13/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.6646 - accuracy: 0.5844\n","Epoch 13: BER = 0.415571153163910\n","Epoch 13: Accuracy = 0.584428846836090, Validation Accuracy = 0.497971743345261\n","312/312 [==============================] - 5s 16ms/step - loss: 0.6646 - accuracy: 0.5844 - val_loss: 35.6549 - val_accuracy: 0.4980 - lr: 4.0000e-05\n","Epoch 14/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.5844\n","Epoch 14: BER = 0.415577411651611\n","Epoch 14: Accuracy = 0.584422588348389, Validation Accuracy = 0.497846543788910\n","312/312 [==============================] - 5s 17ms/step - loss: 0.6646 - accuracy: 0.5844 - val_loss: 342.1689 - val_accuracy: 0.4978 - lr: 4.0000e-05\n","Epoch 14: early stopping\n","78/78 [==============================] - 0s 4ms/step - loss: 342.0836 - accuracy: 0.4980\n","Model: \"sequential_54\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_216 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_162 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_162 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_217 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_163 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_163 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_218 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_164 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_164 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_219 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6724 - accuracy: 0.5653\n","Epoch 1: BER = 0.434657931327820\n","Epoch 1: Accuracy = 0.565342068672180, Validation Accuracy = 0.499073505401611\n","312/312 [==============================] - 8s 19ms/step - loss: 0.6724 - accuracy: 0.5653 - val_loss: 7.8395 - val_accuracy: 0.4991 - lr: 0.0010\n","Epoch 2/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6648 - accuracy: 0.5857\n","Epoch 2: BER = 0.414244055747986\n","Epoch 2: Accuracy = 0.585755944252014, Validation Accuracy = 0.499173671007156\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6648 - accuracy: 0.5858 - val_loss: 448.3166 - val_accuracy: 0.4992 - lr: 0.0010\n","Epoch 3/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6645 - accuracy: 0.5866\n","Epoch 3: BER = 0.413392663002014\n","Epoch 3: Accuracy = 0.586607336997986, Validation Accuracy = 0.499223768711090\n","312/312 [==============================] - 5s 17ms/step - loss: 0.6645 - accuracy: 0.5866 - val_loss: 1973.7404 - val_accuracy: 0.4992 - lr: 0.0010\n","Epoch 4/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6645 - accuracy: 0.5862\n","Epoch 4: BER = 0.413849651813507\n","Epoch 4: Accuracy = 0.586150348186493, Validation Accuracy = 0.499298870563507\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6645 - accuracy: 0.5862 - val_loss: 907.7845 - val_accuracy: 0.4993 - lr: 0.0010\n","Epoch 5/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6643 - accuracy: 0.5862\n","Epoch 5: BER = 0.413824617862701\n","Epoch 5: Accuracy = 0.586175382137299, Validation Accuracy = 0.499098569154739\n","312/312 [==============================] - 6s 18ms/step - loss: 0.6643 - accuracy: 0.5862 - val_loss: 1168.5092 - val_accuracy: 0.4991 - lr: 0.0010\n","Epoch 6/30\n","312/312 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.5865\n","Epoch 6: BER = 0.413536667823792\n","Epoch 6: Accuracy = 0.586463332176208, Validation Accuracy = 0.499223768711090\n","312/312 [==============================] - 5s 15ms/step - loss: 0.6642 - accuracy: 0.5865 - val_loss: 4055.1912 - val_accuracy: 0.4992 - lr: 0.0010\n","Epoch 7/30\n","308/312 [============================>.] - ETA: 0s - loss: 0.6642 - accuracy: 0.5856\n","Epoch 7: BER = 0.414156377315521\n","Epoch 7: Accuracy = 0.585843622684479, Validation Accuracy = 0.499223768711090\n","312/312 [==============================] - 4s 12ms/step - loss: 0.6642 - accuracy: 0.5858 - val_loss: 54.7200 - val_accuracy: 0.4992 - lr: 2.0000e-04\n","Epoch 8/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6641 - accuracy: 0.5863\n","Epoch 8: BER = 0.413574218750000\n","Epoch 8: Accuracy = 0.586425781250000, Validation Accuracy = 0.499348968267441\n","312/312 [==============================] - 5s 15ms/step - loss: 0.6641 - accuracy: 0.5864 - val_loss: 823.4969 - val_accuracy: 0.4993 - lr: 2.0000e-04\n","Epoch 9/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.6641 - accuracy: 0.5869\n","Epoch 9: BER = 0.413129746913910\n","Epoch 9: Accuracy = 0.586870253086090, Validation Accuracy = 0.499198704957962\n","312/312 [==============================] - 5s 17ms/step - loss: 0.6641 - accuracy: 0.5869 - val_loss: 570.9195 - val_accuracy: 0.4992 - lr: 2.0000e-04\n","Epoch 10/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.6641 - accuracy: 0.5863\n","Epoch 10: BER = 0.413862168788910\n","Epoch 10: Accuracy = 0.586137831211090, Validation Accuracy = 0.499098569154739\n","312/312 [==============================] - 4s 13ms/step - loss: 0.6641 - accuracy: 0.5861 - val_loss: 927.4767 - val_accuracy: 0.4991 - lr: 2.0000e-04\n","Epoch 10: early stopping\n","78/78 [==============================] - 0s 4ms/step - loss: 927.0593 - accuracy: 0.4993\n"]}]},{"cell_type":"code","source":["plt.figure\n","plt.semilogy(Z,Loss)\n","plt.xlabel(\"Z\")\n","plt.ylabel(\"validation Loss\")\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":786},"id":"Juuq3lEii3zn","executionInfo":{"status":"error","timestamp":1718292419877,"user_tz":-120,"elapsed":680,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}},"outputId":"d460dbcf-118c-4a64-e554-90ceaa9d9edb"},"execution_count":32,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"x and y must have same first dimension, but have shapes (1,) and (10,)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-fca3d634cff3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Z\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msemilogy\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2878\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msemilogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2880\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36msemilogy\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                       'basey', 'subsy', 'nonposy']}\n\u001b[1;32m   1922\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         return self.plot(\n\u001b[0m\u001b[1;32m   1924\u001b[0m             *args, **{k: v for k, v in kwargs.items() if k not in d})\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (10,)"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Visualize the BER values\n","plt.plot(range(1, len(ber_callback.ber_values) + 1), ber_callback.ber_values, marker='o')\n","plt.title('BER over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('BER')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"0Dddsez4mAlk","executionInfo":{"status":"ok","timestamp":1718288435171,"user_tz":-120,"elapsed":953,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}},"outputId":"6c9850b9-68d3-4cc5-b473-179c4832b0ff"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"FWCw4Pe2pQwA","executionInfo":{"status":"ok","timestamp":1718282230504,"user_tz":-120,"elapsed":266,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","SNR = np.linspace(0,30,10)\n","# Z = np.linspace(1,9,9)\n","Z = 5\n","P = 0.01\n","clearwater = Watertype(0.551,0)\n","Loss = []\n","Accuracy = []\n","for snr in SNR:\n","\n","  S_1,S_0,Denm_trm1,Denm_trm0,Threshold_optimal = Simulated_dataset_V(snr,Z,P,clearwater,Noise='Gauss')\n","  Label0 = np.zeros(len(S_0))\n","  Label1 = np.ones(len(S_1))\n","  # File path\n","  file_path = 'supervised_dataset.csv'\n","  # Check if the file exists and delete it if it does\n","  if os.path.exists(file_path):\n","      os.remove(file_path)\n","  # Create a DataFrame\n","  data1 = pd.DataFrame({'Feature': S_1, 'Label': Label1})\n","  data0 = pd.DataFrame({'Feature': S_0, 'Label': Label0})\n","  # combine the bit 1 and bit 0 signal\n","  merged_data = pd.concat([data1, data0], ignore_index=True)\n","  # Save the dataset to a CSV file\n","  merged_data.to_csv(file_path, index=False)\n","\n","  data = pd.read_csv(file_path)\n","  shuffled_data = shuffle(data)\n","  # Split the dataset into features (X) and labels (y)\n","  X = shuffled_data[['Feature']].values\n","  Y = shuffled_data['Label'].values\n","  # Split the data into training and testing sets\n","  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","  # Create data generators\n","  batch_size = 512\n","  training_generator = DataGenerator(X_train, Y_train, batch_size=batch_size)\n","  validation_generator = DataGenerator(X_test, Y_test, batch_size=batch_size)\n","  # Adjust input_length to match the number of features\n","  input_length = X_train.shape[1]\n","\n","  # Create the model\n","  CNN0 = create_model(input_length)\n","  # Learning rate scheduler\n","  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n","  # Train the model using the data generator\n","  # history = CNN0.fit(training_generator, validation_data=validation_generator, epochs=30)\n","  history = CNN0.fit(training_generator, validation_data=validation_generator, epochs=30, callbacks=[reduce_lr, ber_callback, early_stopping, CustomCallback()])\n","  # Evaluate the model\n","  loss, accuracy = CNN0.evaluate(validation_generator)\n","\n","  Loss.append(loss)\n","  Accuracy.append(accuracy)\n","\n","  # print(f'Test Loss: {loss:.6f}')\n","  # print(f'Test Accuracy: {accuracy:.6f}')\n","\n","\n"],"metadata":{"id":"Ap0E-kzipadE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718291704549,"user_tz":-120,"elapsed":1686195,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}},"outputId":"5801247b-8e47-4454-d40b-7420f57d7de3"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_35\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_140 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_105 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_105 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_141 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_106 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_106 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_142 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_107 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_107 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_143 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n","Epoch 1: BER = 0.000000000000000\n","Epoch 1: Accuracy = 1.000000000000000, Validation Accuracy = 0.503004789352417\n","312/312 [==============================] - 8s 17ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 0.5030 - lr: 0.0010\n","Epoch 2/30\n","308/312 [============================>.] - ETA: 0s - loss: 2.4044e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 2.3895e-04 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.0003e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 9.9703e-05 - accuracy: 1.0000 - val_loss: 1.0070e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","312/312 [==============================] - ETA: 0s - loss: 5.5809e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 5.5809e-05 - accuracy: 1.0000 - val_loss: 3.2685e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.4407e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 3.4374e-05 - accuracy: 1.0000 - val_loss: 2.1236e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.3418e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 2.3404e-05 - accuracy: 1.0000 - val_loss: 1.5317e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.6678e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.6697e-05 - accuracy: 1.0000 - val_loss: 1.1137e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.2118e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 1.2108e-05 - accuracy: 1.0000 - val_loss: 8.3611e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.0446e-05 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 1.0440e-05 - accuracy: 1.0000 - val_loss: 8.4199e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 10/30\n","309/312 [============================>.] - ETA: 0s - loss: 9.7929e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 9.7906e-06 - accuracy: 1.0000 - val_loss: 7.8351e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 11/30\n","311/312 [============================>.] - ETA: 0s - loss: 9.2029e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 9.1981e-06 - accuracy: 1.0000 - val_loss: 7.2541e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","310/312 [============================>.] - ETA: 0s - loss: 8.1959e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 8.1923e-06 - accuracy: 1.0000 - val_loss: 6.6230e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.6897e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 7.6890e-06 - accuracy: 1.0000 - val_loss: 6.0396e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","307/312 [============================>.] - ETA: 0s - loss: 7.2625e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 7.2522e-06 - accuracy: 1.0000 - val_loss: 6.1224e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 15/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.1349e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 7.1305e-06 - accuracy: 1.0000 - val_loss: 6.0511e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 16/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.9383e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 6.9329e-06 - accuracy: 1.0000 - val_loss: 5.8927e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.7358e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 6.7319e-06 - accuracy: 1.0000 - val_loss: 5.6728e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.6209e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 6.6213e-06 - accuracy: 1.0000 - val_loss: 5.4575e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.5020e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 6.4909e-06 - accuracy: 1.0000 - val_loss: 5.4311e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 20/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.4395e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 6.4336e-06 - accuracy: 1.0000 - val_loss: 5.4601e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 21/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.2526e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 6.2539e-06 - accuracy: 1.0000 - val_loss: 5.3389e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","308/312 [============================>.] - ETA: 0s - loss: 6.1642e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 6.1936e-06 - accuracy: 1.0000 - val_loss: 5.3019e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","308/312 [============================>.] - ETA: 0s - loss: 6.2259e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 6.2113e-06 - accuracy: 1.0000 - val_loss: 5.1613e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.0406e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 6.0424e-06 - accuracy: 1.0000 - val_loss: 5.0611e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.8492e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 5.8947e-06 - accuracy: 1.0000 - val_loss: 4.9660e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.6575e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 5.6573e-06 - accuracy: 1.0000 - val_loss: 4.8487e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.6236e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 3s 11ms/step - loss: 5.6205e-06 - accuracy: 1.0000 - val_loss: 4.8035e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.5354e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 5.5340e-06 - accuracy: 1.0000 - val_loss: 4.5227e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","307/312 [============================>.] - ETA: 0s - loss: 5.4087e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 5.4100e-06 - accuracy: 1.0000 - val_loss: 4.4097e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.2031e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 5.2003e-06 - accuracy: 1.0000 - val_loss: 4.2318e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 4ms/step - loss: 4.2312e-06 - accuracy: 1.0000\n","Model: \"sequential_36\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_144 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_108 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_108 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_145 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_109 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_109 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_146 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_110 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_110 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_147 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9984\n","Epoch 1: BER = 0.001571238040924\n","Epoch 1: Accuracy = 0.998428761959076, Validation Accuracy = 0.499599367380142\n","312/312 [==============================] - 9s 22ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.6424 - val_accuracy: 0.4996 - lr: 0.0010\n","Epoch 2/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.4561e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 2.4492e-04 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","307/312 [============================>.] - ETA: 0s - loss: 1.0870e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.0811e-04 - accuracy: 1.0000 - val_loss: 1.2554e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","312/312 [==============================] - ETA: 0s - loss: 5.9029e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 5.9029e-05 - accuracy: 1.0000 - val_loss: 3.2646e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","308/312 [============================>.] - ETA: 0s - loss: 3.8056e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 3.8077e-05 - accuracy: 1.0000 - val_loss: 2.5307e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","312/312 [==============================] - ETA: 0s - loss: 2.5670e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 2.5670e-05 - accuracy: 1.0000 - val_loss: 1.6337e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.8128e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.8092e-05 - accuracy: 1.0000 - val_loss: 1.2747e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","312/312 [==============================] - ETA: 0s - loss: 1.3380e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 20ms/step - loss: 1.3380e-05 - accuracy: 1.0000 - val_loss: 1.0051e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.0282e-05 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.0257e-05 - accuracy: 1.0000 - val_loss: 7.9846e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","310/312 [============================>.] - ETA: 0s - loss: 8.0047e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 7.9976e-06 - accuracy: 1.0000 - val_loss: 5.5180e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 11/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.8630e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 6.8663e-06 - accuracy: 1.0000 - val_loss: 5.5505e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.4895e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.4911e-06 - accuracy: 1.0000 - val_loss: 5.2466e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.0721e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 6.0715e-06 - accuracy: 1.0000 - val_loss: 4.9387e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.6682e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 5.6704e-06 - accuracy: 1.0000 - val_loss: 4.6412e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.2640e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 5.3142e-06 - accuracy: 1.0000 - val_loss: 4.2272e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 16/30\n","312/312 [==============================] - ETA: 0s - loss: 5.0038e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 5.0038e-06 - accuracy: 1.0000 - val_loss: 4.2371e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.8448e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.8397e-06 - accuracy: 1.0000 - val_loss: 4.1608e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.9103e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 4.9223e-06 - accuracy: 1.0000 - val_loss: 4.0617e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.7013e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.6993e-06 - accuracy: 1.0000 - val_loss: 3.9398e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.5304e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 4.5325e-06 - accuracy: 1.0000 - val_loss: 3.8347e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 21/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.6204e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 11ms/step - loss: 4.6187e-06 - accuracy: 1.0000 - val_loss: 3.8083e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","312/312 [==============================] - ETA: 0s - loss: 4.4273e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.4273e-06 - accuracy: 1.0000 - val_loss: 3.7748e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.4971e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.4949e-06 - accuracy: 1.0000 - val_loss: 3.7268e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.3492e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.3520e-06 - accuracy: 1.0000 - val_loss: 3.6915e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","312/312 [==============================] - ETA: 0s - loss: 4.3188e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 4.3188e-06 - accuracy: 1.0000 - val_loss: 3.6238e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.1984e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.1972e-06 - accuracy: 1.0000 - val_loss: 3.5600e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","312/312 [==============================] - ETA: 0s - loss: 4.1396e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.1396e-06 - accuracy: 1.0000 - val_loss: 3.4888e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","312/312 [==============================] - ETA: 0s - loss: 4.0933e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 4.0933e-06 - accuracy: 1.0000 - val_loss: 3.4271e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.9734e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 3.9729e-06 - accuracy: 1.0000 - val_loss: 3.3212e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","312/312 [==============================] - ETA: 0s - loss: 3.9104e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 3.9104e-06 - accuracy: 1.0000 - val_loss: 3.2429e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 4ms/step - loss: 3.2430e-06 - accuracy: 1.0000\n","Model: \"sequential_37\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_148 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_111 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_111 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_149 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_112 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_112 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_150 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_113 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_113 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_151 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9968\n","Epoch 1: BER = 0.003205120563507\n","Epoch 1: Accuracy = 0.996794879436493, Validation Accuracy = 0.500450730323792\n","312/312 [==============================] - 8s 19ms/step - loss: 0.0064 - accuracy: 0.9968 - val_loss: 0.6408 - val_accuracy: 0.5005 - lr: 0.0010\n","Epoch 2/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.2512e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 3.2470e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","312/312 [==============================] - ETA: 0s - loss: 1.4561e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.4561e-04 - accuracy: 1.0000 - val_loss: 1.7092e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","310/312 [============================>.] - ETA: 0s - loss: 8.3540e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 8.3384e-05 - accuracy: 1.0000 - val_loss: 5.0519e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.2112e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 5.2105e-05 - accuracy: 1.0000 - val_loss: 3.4774e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.6495e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 3.6465e-05 - accuracy: 1.0000 - val_loss: 2.4870e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.5930e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 2.5888e-05 - accuracy: 1.0000 - val_loss: 1.8042e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.8764e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 1.8748e-05 - accuracy: 1.0000 - val_loss: 1.3300e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.4558e-05 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 1.4547e-05 - accuracy: 1.0000 - val_loss: 1.0267e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.2247e-05 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.2257e-05 - accuracy: 1.0000 - val_loss: 1.0225e-05 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 11/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.1553e-05 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.1552e-05 - accuracy: 1.0000 - val_loss: 9.5288e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.0795e-05 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 20ms/step - loss: 1.0787e-05 - accuracy: 1.0000 - val_loss: 8.9888e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.0247e-05 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.0236e-05 - accuracy: 1.0000 - val_loss: 8.2789e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","308/312 [============================>.] - ETA: 0s - loss: 9.2561e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 9.2581e-06 - accuracy: 1.0000 - val_loss: 7.5877e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","312/312 [==============================] - ETA: 0s - loss: 8.9221e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 8.9221e-06 - accuracy: 1.0000 - val_loss: 7.5880e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 16/30\n","312/312 [==============================] - ETA: 0s - loss: 8.6307e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 8.6307e-06 - accuracy: 1.0000 - val_loss: 7.4416e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","312/312 [==============================] - ETA: 0s - loss: 8.6360e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 8.6360e-06 - accuracy: 1.0000 - val_loss: 7.2551e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","310/312 [============================>.] - ETA: 0s - loss: 8.2355e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 8.2354e-06 - accuracy: 1.0000 - val_loss: 7.0464e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.9398e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 7.9396e-06 - accuracy: 1.0000 - val_loss: 6.8350e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.9229e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 7.9165e-06 - accuracy: 1.0000 - val_loss: 6.8036e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 21/30\n","308/312 [============================>.] - ETA: 0s - loss: 7.9405e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.9389e-06 - accuracy: 1.0000 - val_loss: 6.7133e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.7749e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 20ms/step - loss: 7.7716e-06 - accuracy: 1.0000 - val_loss: 6.6436e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","308/312 [============================>.] - ETA: 0s - loss: 7.5369e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 7.5360e-06 - accuracy: 1.0000 - val_loss: 6.5726e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.7611e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 7.7665e-06 - accuracy: 1.0000 - val_loss: 6.4500e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","308/312 [============================>.] - ETA: 0s - loss: 7.4736e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 7.4730e-06 - accuracy: 1.0000 - val_loss: 6.3323e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.2210e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.2298e-06 - accuracy: 1.0000 - val_loss: 6.1993e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","312/312 [==============================] - ETA: 0s - loss: 7.1389e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.1389e-06 - accuracy: 1.0000 - val_loss: 6.0755e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.0514e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 20ms/step - loss: 7.0457e-06 - accuracy: 1.0000 - val_loss: 5.9217e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.8452e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.8400e-06 - accuracy: 1.0000 - val_loss: 5.7330e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.6036e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 6.6175e-06 - accuracy: 1.0000 - val_loss: 5.5444e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 1s 7ms/step - loss: 5.5444e-06 - accuracy: 1.0000\n","Model: \"sequential_38\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_152 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_114 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_114 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_153 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_115 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_115 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_154 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_116 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_116 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_155 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","311/312 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9984\n","Epoch 1: BER = 0.001590073108673\n","Epoch 1: Accuracy = 0.998409926891327, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 13ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.6357 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 2/30\n","308/312 [============================>.] - ETA: 0s - loss: 2.6577e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.6420e-04 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.0917e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 1.0886e-04 - accuracy: 1.0000 - val_loss: 1.0677e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","312/312 [==============================] - ETA: 0s - loss: 5.7764e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 5.7764e-05 - accuracy: 1.0000 - val_loss: 3.4954e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","312/312 [==============================] - ETA: 0s - loss: 3.5624e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 3.5624e-05 - accuracy: 1.0000 - val_loss: 2.2358e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.3740e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 2.3707e-05 - accuracy: 1.0000 - val_loss: 1.5212e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.6666e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.6627e-05 - accuracy: 1.0000 - val_loss: 1.1060e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.1998e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.1974e-05 - accuracy: 1.0000 - val_loss: 8.2815e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","312/312 [==============================] - ETA: 0s - loss: 9.9753e-06 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 9.9753e-06 - accuracy: 1.0000 - val_loss: 8.2671e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 10/30\n","309/312 [============================>.] - ETA: 0s - loss: 9.5807e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 9.5702e-06 - accuracy: 1.0000 - val_loss: 7.7591e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 11/30\n","311/312 [============================>.] - ETA: 0s - loss: 8.9138e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 8.9089e-06 - accuracy: 1.0000 - val_loss: 7.1136e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","310/312 [============================>.] - ETA: 0s - loss: 8.0590e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 8.0530e-06 - accuracy: 1.0000 - val_loss: 6.5477e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","308/312 [============================>.] - ETA: 0s - loss: 7.2995e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 7.2989e-06 - accuracy: 1.0000 - val_loss: 6.0032e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","312/312 [==============================] - ETA: 0s - loss: 6.7979e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.7979e-06 - accuracy: 1.0000 - val_loss: 5.4269e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","312/312 [==============================] - ETA: 0s - loss: 6.1196e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 6.1196e-06 - accuracy: 1.0000 - val_loss: 4.8696e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 16/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.4916e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 5.4872e-06 - accuracy: 1.0000 - val_loss: 4.3354e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 17/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.9387e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.9300e-06 - accuracy: 1.0000 - val_loss: 3.8569e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 18/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.5394e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.5416e-06 - accuracy: 1.0000 - val_loss: 3.8642e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","312/312 [==============================] - ETA: 0s - loss: 4.4413e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.4413e-06 - accuracy: 1.0000 - val_loss: 3.7467e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","312/312 [==============================] - ETA: 0s - loss: 4.3646e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.3646e-06 - accuracy: 1.0000 - val_loss: 3.6191e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 21/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.1313e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.1350e-06 - accuracy: 1.0000 - val_loss: 3.4843e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 22/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.0133e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.0230e-06 - accuracy: 1.0000 - val_loss: 3.3392e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 23/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.9169e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 3.9151e-06 - accuracy: 1.0000 - val_loss: 3.3282e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","312/312 [==============================] - ETA: 0s - loss: 3.8764e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 3.8764e-06 - accuracy: 1.0000 - val_loss: 3.2877e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","312/312 [==============================] - ETA: 0s - loss: 3.8304e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 3.8304e-06 - accuracy: 1.0000 - val_loss: 3.2324e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.7437e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 3.7426e-06 - accuracy: 1.0000 - val_loss: 3.1723e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.6922e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 3.6937e-06 - accuracy: 1.0000 - val_loss: 3.1077e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","312/312 [==============================] - ETA: 0s - loss: 3.5765e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.5765e-06 - accuracy: 1.0000 - val_loss: 3.0430e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.5499e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.5453e-06 - accuracy: 1.0000 - val_loss: 2.9615e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.4438e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 20ms/step - loss: 3.4416e-06 - accuracy: 1.0000 - val_loss: 2.8778e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 4ms/step - loss: 2.8778e-06 - accuracy: 1.0000\n","Model: \"sequential_39\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_156 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_117 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_117 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_157 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_118 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_118 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_158 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_119 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_119 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_159 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","309/312 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n","Epoch 1: BER = 0.000000000000000\n","Epoch 1: Accuracy = 1.000000000000000, Validation Accuracy = 0.497721344232559\n","312/312 [==============================] - 6s 13ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 0.4977 - lr: 0.0010\n","Epoch 2/30\n","312/312 [==============================] - ETA: 0s - loss: 2.2716e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 2.2716e-04 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","312/312 [==============================] - ETA: 0s - loss: 9.8275e-05 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 9.8275e-05 - accuracy: 1.0000 - val_loss: 1.4170e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.3315e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 5.3285e-05 - accuracy: 1.0000 - val_loss: 3.5651e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","308/312 [============================>.] - ETA: 0s - loss: 3.1185e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.1087e-05 - accuracy: 1.0000 - val_loss: 4.5022e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.1371e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 2.1354e-05 - accuracy: 1.0000 - val_loss: 1.3974e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.5792e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.5769e-05 - accuracy: 1.0000 - val_loss: 2.2871e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.1513e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.1514e-05 - accuracy: 1.0000 - val_loss: 7.2549e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","311/312 [============================>.] - ETA: 0s - loss: 8.8008e-06 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 8.7931e-06 - accuracy: 1.0000 - val_loss: 6.5613e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","311/312 [============================>.] - ETA: 0s - loss: 7.5077e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 7.5127e-06 - accuracy: 1.0000 - val_loss: 5.8150e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 11/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.0184e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 7.0086e-06 - accuracy: 1.0000 - val_loss: 5.7169e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","312/312 [==============================] - ETA: 0s - loss: 6.8304e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.8304e-06 - accuracy: 1.0000 - val_loss: 5.5406e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","312/312 [==============================] - ETA: 0s - loss: 6.1794e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 6.1794e-06 - accuracy: 1.0000 - val_loss: 4.9397e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.7605e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 5.7573e-06 - accuracy: 1.0000 - val_loss: 4.8493e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","308/312 [============================>.] - ETA: 0s - loss: 5.4770e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 5.4826e-06 - accuracy: 1.0000 - val_loss: 4.6723e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 16/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.3132e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 5.3280e-06 - accuracy: 1.0000 - val_loss: 4.5871e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.2203e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 5.2218e-06 - accuracy: 1.0000 - val_loss: 4.4438e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","309/312 [============================>.] - ETA: 0s - loss: 5.1500e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 5.1426e-06 - accuracy: 1.0000 - val_loss: 4.2534e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.0490e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 5.0482e-06 - accuracy: 1.0000 - val_loss: 4.2607e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","312/312 [==============================] - ETA: 0s - loss: 4.8821e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.8821e-06 - accuracy: 1.0000 - val_loss: 4.1687e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 21/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.7980e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.8070e-06 - accuracy: 1.0000 - val_loss: 4.1172e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.8092e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 4.8111e-06 - accuracy: 1.0000 - val_loss: 4.0743e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.7906e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.7870e-06 - accuracy: 1.0000 - val_loss: 4.0933e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","312/312 [==============================] - ETA: 0s - loss: 4.6552e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.6552e-06 - accuracy: 1.0000 - val_loss: 3.9563e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.5988e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 4.6035e-06 - accuracy: 1.0000 - val_loss: 3.8399e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.5832e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 4.5856e-06 - accuracy: 1.0000 - val_loss: 3.7892e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","312/312 [==============================] - ETA: 0s - loss: 4.4504e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 4.4504e-06 - accuracy: 1.0000 - val_loss: 3.7084e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.3257e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.3213e-06 - accuracy: 1.0000 - val_loss: 3.6935e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.2577e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 4.2634e-06 - accuracy: 1.0000 - val_loss: 3.5096e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","312/312 [==============================] - ETA: 0s - loss: 4.1032e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.1032e-06 - accuracy: 1.0000 - val_loss: 3.4167e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 1s 6ms/step - loss: 3.4167e-06 - accuracy: 1.0000\n","Model: \"sequential_40\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_160 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_120 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_120 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_161 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_121 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_121 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_162 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_122 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_122 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_163 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9984\n","Epoch 1: BER = 0.001571238040924\n","Epoch 1: Accuracy = 0.998428761959076, Validation Accuracy = 0.500400662422180\n","312/312 [==============================] - 7s 15ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.6273 - val_accuracy: 0.5004 - lr: 0.0010\n","Epoch 2/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.3207e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 3.3175e-04 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.3713e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 1.3680e-04 - accuracy: 1.0000 - val_loss: 1.4328e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","308/312 [============================>.] - ETA: 0s - loss: 7.3490e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 7.3384e-05 - accuracy: 1.0000 - val_loss: 4.8331e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.5370e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.5299e-05 - accuracy: 1.0000 - val_loss: 3.0690e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.0336e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.0287e-05 - accuracy: 1.0000 - val_loss: 2.0952e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","312/312 [==============================] - ETA: 0s - loss: 2.1113e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 2.1113e-05 - accuracy: 1.0000 - val_loss: 1.4870e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.5596e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 1.5576e-05 - accuracy: 1.0000 - val_loss: 1.1079e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.1530e-05 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.1511e-05 - accuracy: 1.0000 - val_loss: 8.4149e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","311/312 [============================>.] - ETA: 0s - loss: 8.9748e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 8.9747e-06 - accuracy: 1.0000 - val_loss: 6.4179e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 11/30\n","312/312 [==============================] - ETA: 0s - loss: 7.5640e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 7.5640e-06 - accuracy: 1.0000 - val_loss: 6.3153e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","312/312 [==============================] - ETA: 0s - loss: 7.1259e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 7.1259e-06 - accuracy: 1.0000 - val_loss: 5.9002e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","312/312 [==============================] - ETA: 0s - loss: 6.6384e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.6384e-06 - accuracy: 1.0000 - val_loss: 5.6087e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","312/312 [==============================] - ETA: 0s - loss: 6.2619e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 6.2619e-06 - accuracy: 1.0000 - val_loss: 5.1956e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","309/312 [============================>.] - ETA: 0s - loss: 5.8331e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 5.8239e-06 - accuracy: 1.0000 - val_loss: 4.7388e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 16/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.5309e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 5.5258e-06 - accuracy: 1.0000 - val_loss: 4.7055e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","309/312 [============================>.] - ETA: 0s - loss: 5.4486e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 5.4480e-06 - accuracy: 1.0000 - val_loss: 4.6454e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.2084e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 5.2127e-06 - accuracy: 1.0000 - val_loss: 4.5296e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.1796e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 5.1766e-06 - accuracy: 1.0000 - val_loss: 4.4050e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.0810e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 5.0797e-06 - accuracy: 1.0000 - val_loss: 4.2649e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 21/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.9319e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 4.9348e-06 - accuracy: 1.0000 - val_loss: 4.2523e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.9818e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.9811e-06 - accuracy: 1.0000 - val_loss: 4.1928e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.7930e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.8016e-06 - accuracy: 1.0000 - val_loss: 4.1779e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.7672e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 4.7672e-06 - accuracy: 1.0000 - val_loss: 4.0753e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","312/312 [==============================] - ETA: 0s - loss: 4.6603e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.6603e-06 - accuracy: 1.0000 - val_loss: 4.0307e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.6905e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.6891e-06 - accuracy: 1.0000 - val_loss: 3.9802e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.5286e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.5272e-06 - accuracy: 1.0000 - val_loss: 3.8641e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.4473e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 20ms/step - loss: 4.4462e-06 - accuracy: 1.0000 - val_loss: 3.7775e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.3309e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.3271e-06 - accuracy: 1.0000 - val_loss: 3.6923e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","309/312 [============================>.] - ETA: 0s - loss: 4.3039e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.3028e-06 - accuracy: 1.0000 - val_loss: 3.5851e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 5ms/step - loss: 3.5851e-06 - accuracy: 1.0000\n","Model: \"sequential_41\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_164 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_123 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_123 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_165 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_124 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_124 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_166 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_125 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_125 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_167 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9984\n","Epoch 1: BER = 0.001546204090118\n","Epoch 1: Accuracy = 0.998453795909882, Validation Accuracy = 0.500150263309479\n","312/312 [==============================] - 8s 17ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.6343 - val_accuracy: 0.5002 - lr: 0.0010\n","Epoch 2/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.0546e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 4.0432e-04 - accuracy: 1.0000 - val_loss: 0.0349 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","308/312 [============================>.] - ETA: 0s - loss: 1.6160e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 1.6103e-04 - accuracy: 1.0000 - val_loss: 1.9490e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","309/312 [============================>.] - ETA: 0s - loss: 8.7332e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 8.7177e-05 - accuracy: 1.0000 - val_loss: 5.2677e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","308/312 [============================>.] - ETA: 0s - loss: 5.3587e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 5.3394e-05 - accuracy: 1.0000 - val_loss: 3.8130e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","312/312 [==============================] - ETA: 0s - loss: 3.5690e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 3.5690e-05 - accuracy: 1.0000 - val_loss: 2.4300e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.5306e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 2.5285e-05 - accuracy: 1.0000 - val_loss: 1.8084e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.8200e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.8173e-05 - accuracy: 1.0000 - val_loss: 1.3898e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.3935e-05 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 1.3907e-05 - accuracy: 1.0000 - val_loss: 1.0001e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","312/312 [==============================] - ETA: 0s - loss: 1.1741e-05 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 1.1741e-05 - accuracy: 1.0000 - val_loss: 9.9315e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 11/30\n","312/312 [==============================] - ETA: 0s - loss: 1.0930e-05 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.0930e-05 - accuracy: 1.0000 - val_loss: 9.3489e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.0370e-05 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 1.0371e-05 - accuracy: 1.0000 - val_loss: 8.6699e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","309/312 [============================>.] - ETA: 0s - loss: 9.8736e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 9.8754e-06 - accuracy: 1.0000 - val_loss: 8.0227e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","308/312 [============================>.] - ETA: 0s - loss: 9.0383e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 9.0264e-06 - accuracy: 1.0000 - val_loss: 7.2701e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","309/312 [============================>.] - ETA: 0s - loss: 8.3451e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 8.3475e-06 - accuracy: 1.0000 - val_loss: 7.2600e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 16/30\n","312/312 [==============================] - ETA: 0s - loss: 8.3123e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 8.3123e-06 - accuracy: 1.0000 - val_loss: 7.1824e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","309/312 [============================>.] - ETA: 0s - loss: 8.2262e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 8.2155e-06 - accuracy: 1.0000 - val_loss: 6.9442e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","309/312 [============================>.] - ETA: 0s - loss: 8.0080e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 8.0170e-06 - accuracy: 1.0000 - val_loss: 6.7820e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","311/312 [============================>.] - ETA: 0s - loss: 7.6846e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 7.6826e-06 - accuracy: 1.0000 - val_loss: 6.5486e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.4607e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.4551e-06 - accuracy: 1.0000 - val_loss: 6.5572e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 21/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.4618e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 7.4565e-06 - accuracy: 1.0000 - val_loss: 6.4618e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.2728e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 18ms/step - loss: 7.2707e-06 - accuracy: 1.0000 - val_loss: 6.3795e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.2092e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.2070e-06 - accuracy: 1.0000 - val_loss: 6.2912e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","308/312 [============================>.] - ETA: 0s - loss: 7.1057e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.1005e-06 - accuracy: 1.0000 - val_loss: 6.1868e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","310/312 [============================>.] - ETA: 0s - loss: 7.1541e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 7.1497e-06 - accuracy: 1.0000 - val_loss: 6.0881e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.9057e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 6.9055e-06 - accuracy: 1.0000 - val_loss: 5.9650e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","308/312 [============================>.] - ETA: 0s - loss: 6.7621e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.7573e-06 - accuracy: 1.0000 - val_loss: 5.8289e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.7564e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 6.7804e-06 - accuracy: 1.0000 - val_loss: 5.6245e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.4433e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 6.4459e-06 - accuracy: 1.0000 - val_loss: 5.4569e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.2535e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.2582e-06 - accuracy: 1.0000 - val_loss: 5.3273e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 5ms/step - loss: 5.3270e-06 - accuracy: 1.0000\n","Model: \"sequential_42\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_168 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_126 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_126 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_169 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_127 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_127 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_170 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_128 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_128 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_171 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9984\n","Epoch 1: BER = 0.001621365547180\n","Epoch 1: Accuracy = 0.998378634452820, Validation Accuracy = 0.496819913387299\n","312/312 [==============================] - 8s 17ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.6325 - val_accuracy: 0.4968 - lr: 0.0010\n","Epoch 2/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.5035e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.4968e-04 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","309/312 [============================>.] - ETA: 0s - loss: 1.0382e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 1.0351e-04 - accuracy: 1.0000 - val_loss: 1.1368e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","309/312 [============================>.] - ETA: 0s - loss: 5.5657e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 5.5521e-05 - accuracy: 1.0000 - val_loss: 3.4267e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","312/312 [==============================] - ETA: 0s - loss: 3.4329e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 3.4329e-05 - accuracy: 1.0000 - val_loss: 2.3011e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","310/312 [============================>.] - ETA: 0s - loss: 2.3153e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 2.3133e-05 - accuracy: 1.0000 - val_loss: 1.5867e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","312/312 [==============================] - ETA: 0s - loss: 1.6599e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.6599e-05 - accuracy: 1.0000 - val_loss: 1.1410e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","312/312 [==============================] - ETA: 0s - loss: 1.2093e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.2093e-05 - accuracy: 1.0000 - val_loss: 8.5162e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","309/312 [============================>.] - ETA: 0s - loss: 9.0212e-06 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 9.0101e-06 - accuracy: 1.0000 - val_loss: 6.4466e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.9657e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.9588e-06 - accuracy: 1.0000 - val_loss: 5.0569e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 11/30\n","312/312 [==============================] - ETA: 0s - loss: 5.5721e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 5.5721e-06 - accuracy: 1.0000 - val_loss: 3.9414e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 12/30\n","312/312 [==============================] - ETA: 0s - loss: 4.2756e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.2756e-06 - accuracy: 1.0000 - val_loss: 3.1554e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 13/30\n","308/312 [============================>.] - ETA: 0s - loss: 3.8018e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.7964e-06 - accuracy: 1.0000 - val_loss: 3.1016e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","312/312 [==============================] - ETA: 0s - loss: 3.5733e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.5733e-06 - accuracy: 1.0000 - val_loss: 2.9421e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.4929e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 3.4873e-06 - accuracy: 1.0000 - val_loss: 2.7582e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 16/30\n","312/312 [==============================] - ETA: 0s - loss: 3.1818e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.1818e-06 - accuracy: 1.0000 - val_loss: 2.5817e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 17/30\n","312/312 [==============================] - ETA: 0s - loss: 2.9967e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.9967e-06 - accuracy: 1.0000 - val_loss: 2.3941e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 18/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.8387e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 2.8381e-06 - accuracy: 1.0000 - val_loss: 2.3910e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.8177e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 12ms/step - loss: 2.8239e-06 - accuracy: 1.0000 - val_loss: 2.3386e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.7701e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 2.7700e-06 - accuracy: 1.0000 - val_loss: 2.2859e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 21/30\n","308/312 [============================>.] - ETA: 0s - loss: 2.6864e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 2.6861e-06 - accuracy: 1.0000 - val_loss: 2.2355e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 22/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.6193e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.6180e-06 - accuracy: 1.0000 - val_loss: 2.1660e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 23/30\n","308/312 [============================>.] - ETA: 0s - loss: 2.5869e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.5820e-06 - accuracy: 1.0000 - val_loss: 2.1572e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.5638e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 2.5604e-06 - accuracy: 1.0000 - val_loss: 2.1400e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","308/312 [============================>.] - ETA: 0s - loss: 2.4946e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.4979e-06 - accuracy: 1.0000 - val_loss: 2.1179e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.5050e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 18ms/step - loss: 2.5068e-06 - accuracy: 1.0000 - val_loss: 2.0907e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","312/312 [==============================] - ETA: 0s - loss: 2.4589e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.4589e-06 - accuracy: 1.0000 - val_loss: 2.0620e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.4526e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 2.4514e-06 - accuracy: 1.0000 - val_loss: 2.0277e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","312/312 [==============================] - ETA: 0s - loss: 2.3921e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.3921e-06 - accuracy: 1.0000 - val_loss: 2.0066e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","311/312 [============================>.] - ETA: 0s - loss: 2.3367e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 2.3364e-06 - accuracy: 1.0000 - val_loss: 1.9486e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 5ms/step - loss: 1.9486e-06 - accuracy: 1.0000\n","Model: \"sequential_43\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_172 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_129 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_129 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_173 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_130 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_130 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_174 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_131 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_131 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_175 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","310/312 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9968\n","Epoch 1: BER = 0.003205120563507\n","Epoch 1: Accuracy = 0.996794879436493, Validation Accuracy = 0.502754390239716\n","312/312 [==============================] - 7s 17ms/step - loss: 0.0058 - accuracy: 0.9968 - val_loss: 0.6613 - val_accuracy: 0.5028 - lr: 0.0010\n","Epoch 2/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.8345e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 2.8226e-04 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","311/312 [============================>.] - ETA: 0s - loss: 1.1200e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 1.1187e-04 - accuracy: 1.0000 - val_loss: 1.2658e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","311/312 [============================>.] - ETA: 0s - loss: 5.9983e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 5.9987e-05 - accuracy: 1.0000 - val_loss: 3.8710e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","308/312 [============================>.] - ETA: 0s - loss: 3.6933e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 3.6810e-05 - accuracy: 1.0000 - val_loss: 2.3451e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","309/312 [============================>.] - ETA: 0s - loss: 2.4868e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.4895e-05 - accuracy: 1.0000 - val_loss: 1.6793e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","312/312 [==============================] - ETA: 0s - loss: 1.7590e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 1.7590e-05 - accuracy: 1.0000 - val_loss: 1.2019e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","312/312 [==============================] - ETA: 0s - loss: 1.2923e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 1.2923e-05 - accuracy: 1.0000 - val_loss: 9.1923e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","311/312 [============================>.] - ETA: 0s - loss: 9.4675e-06 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 9.4660e-06 - accuracy: 1.0000 - val_loss: 6.9687e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","312/312 [==============================] - ETA: 0s - loss: 7.4237e-06 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.4237e-06 - accuracy: 1.0000 - val_loss: 5.2962e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 11/30\n","312/312 [==============================] - ETA: 0s - loss: 6.4289e-06 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 6.4289e-06 - accuracy: 1.0000 - val_loss: 5.2825e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","312/312 [==============================] - ETA: 0s - loss: 6.0292e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 6.0292e-06 - accuracy: 1.0000 - val_loss: 4.9903e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","310/312 [============================>.] - ETA: 0s - loss: 5.6919e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 5.6932e-06 - accuracy: 1.0000 - val_loss: 4.6391e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","312/312 [==============================] - ETA: 0s - loss: 5.2605e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 5.2605e-06 - accuracy: 1.0000 - val_loss: 4.3453e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","309/312 [============================>.] - ETA: 0s - loss: 5.0605e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 5.0517e-06 - accuracy: 1.0000 - val_loss: 3.9796e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 16/30\n","312/312 [==============================] - ETA: 0s - loss: 4.6462e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.6462e-06 - accuracy: 1.0000 - val_loss: 3.9709e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","312/312 [==============================] - ETA: 0s - loss: 4.5943e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.5943e-06 - accuracy: 1.0000 - val_loss: 3.9011e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","310/312 [============================>.] - ETA: 0s - loss: 4.4728e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.4712e-06 - accuracy: 1.0000 - val_loss: 3.8033e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","312/312 [==============================] - ETA: 0s - loss: 4.4241e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 4.4241e-06 - accuracy: 1.0000 - val_loss: 3.7028e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","312/312 [==============================] - ETA: 0s - loss: 4.2329e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 4.2329e-06 - accuracy: 1.0000 - val_loss: 3.5819e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 21/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.2453e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 4.2433e-06 - accuracy: 1.0000 - val_loss: 3.5565e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.2051e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 4.2067e-06 - accuracy: 1.0000 - val_loss: 3.5207e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","311/312 [============================>.] - ETA: 0s - loss: 4.2096e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.2215e-06 - accuracy: 1.0000 - val_loss: 3.4769e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","308/312 [============================>.] - ETA: 0s - loss: 4.1081e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 4.1084e-06 - accuracy: 1.0000 - val_loss: 3.4325e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","312/312 [==============================] - ETA: 0s - loss: 4.0617e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 4.0617e-06 - accuracy: 1.0000 - val_loss: 3.3867e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.9272e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 3.9237e-06 - accuracy: 1.0000 - val_loss: 3.3258e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","312/312 [==============================] - ETA: 0s - loss: 3.9241e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 3.9241e-06 - accuracy: 1.0000 - val_loss: 3.2552e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","309/312 [============================>.] - ETA: 0s - loss: 3.7625e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 3.7586e-06 - accuracy: 1.0000 - val_loss: 3.1995e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.7010e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 3.7001e-06 - accuracy: 1.0000 - val_loss: 3.1150e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.6137e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 3.6112e-06 - accuracy: 1.0000 - val_loss: 3.0237e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 0s 5ms/step - loss: 3.0238e-06 - accuracy: 1.0000\n","Model: \"sequential_44\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_176 (Dense)           (None, 256)               512       \n","                                                                 \n"," batch_normalization_132 (B  (None, 256)               1024      \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_132 (ReLU)            (None, 256)               0         \n","                                                                 \n"," dense_177 (Dense)           (None, 96)                24672     \n","                                                                 \n"," batch_normalization_133 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_133 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_178 (Dense)           (None, 96)                9312      \n","                                                                 \n"," batch_normalization_134 (B  (None, 96)                384       \n"," atchNormalization)                                              \n","                                                                 \n"," re_lu_134 (ReLU)            (None, 96)                0         \n","                                                                 \n"," dense_179 (Dense)           (None, 1)                 97        \n","                                                                 \n","=================================================================\n","Total params: 36385 (142.13 KB)\n","Trainable params: 35489 (138.63 KB)\n","Non-trainable params: 896 (3.50 KB)\n","_________________________________________________________________\n","Epoch 1/30\n","312/312 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9968\n","Epoch 1: BER = 0.003205120563507\n","Epoch 1: Accuracy = 0.996794879436493, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 14ms/step - loss: 0.0082 - accuracy: 0.9968 - val_loss: 0.6257 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 2/30\n","310/312 [============================>.] - ETA: 0s - loss: 3.9387e-04 - accuracy: 1.0000\n","Epoch 2: BER = 0.000000000000000\n","Epoch 2: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 3.9273e-04 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 3/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.5828e-04 - accuracy: 1.0000\n","Epoch 3: BER = 0.000000000000000\n","Epoch 3: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 1.5805e-04 - accuracy: 1.0000 - val_loss: 1.6282e-04 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 4/30\n","309/312 [============================>.] - ETA: 0s - loss: 8.4617e-05 - accuracy: 1.0000\n","Epoch 4: BER = 0.000000000000000\n","Epoch 4: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 8.4383e-05 - accuracy: 1.0000 - val_loss: 5.5912e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 5/30\n","309/312 [============================>.] - ETA: 0s - loss: 5.2161e-05 - accuracy: 1.0000\n","Epoch 5: BER = 0.000000000000000\n","Epoch 5: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 5.2074e-05 - accuracy: 1.0000 - val_loss: 3.3700e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 6/30\n","311/312 [============================>.] - ETA: 0s - loss: 3.5015e-05 - accuracy: 1.0000\n","Epoch 6: BER = 0.000000000000000\n","Epoch 6: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 3.4998e-05 - accuracy: 1.0000 - val_loss: 2.3809e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 7/30\n","312/312 [==============================] - ETA: 0s - loss: 2.4255e-05 - accuracy: 1.0000\n","Epoch 7: BER = 0.000000000000000\n","Epoch 7: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 2.4255e-05 - accuracy: 1.0000 - val_loss: 1.7182e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 8/30\n","312/312 [==============================] - ETA: 0s - loss: 1.7748e-05 - accuracy: 1.0000\n","Epoch 8: BER = 0.000000000000000\n","Epoch 8: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 1.7748e-05 - accuracy: 1.0000 - val_loss: 1.2941e-05 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 9/30\n","310/312 [============================>.] - ETA: 0s - loss: 1.3406e-05 - accuracy: 1.0000\n","Epoch 9: BER = 0.000000000000000\n","Epoch 9: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 14ms/step - loss: 1.3392e-05 - accuracy: 1.0000 - val_loss: 9.7080e-06 - val_accuracy: 1.0000 - lr: 0.0010\n","Epoch 10/30\n","312/312 [==============================] - ETA: 0s - loss: 1.1215e-05 - accuracy: 1.0000\n","Epoch 10: BER = 0.000000000000000\n","Epoch 10: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 1.1215e-05 - accuracy: 1.0000 - val_loss: 9.6529e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 11/30\n","312/312 [==============================] - ETA: 0s - loss: 1.0644e-05 - accuracy: 1.0000\n","Epoch 11: BER = 0.000000000000000\n","Epoch 11: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 1.0644e-05 - accuracy: 1.0000 - val_loss: 8.9987e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 12/30\n","308/312 [============================>.] - ETA: 0s - loss: 9.9035e-06 - accuracy: 1.0000\n","Epoch 12: BER = 0.000000000000000\n","Epoch 12: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 9.8893e-06 - accuracy: 1.0000 - val_loss: 8.3875e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 13/30\n","311/312 [============================>.] - ETA: 0s - loss: 9.3518e-06 - accuracy: 1.0000\n","Epoch 13: BER = 0.000000000000000\n","Epoch 13: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 9.3471e-06 - accuracy: 1.0000 - val_loss: 7.6580e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 14/30\n","312/312 [==============================] - ETA: 0s - loss: 8.5845e-06 - accuracy: 1.0000\n","Epoch 14: BER = 0.000000000000000\n","Epoch 14: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 8.5845e-06 - accuracy: 1.0000 - val_loss: 7.1485e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n","Epoch 15/30\n","310/312 [============================>.] - ETA: 0s - loss: 8.0180e-06 - accuracy: 1.0000\n","Epoch 15: BER = 0.000000000000000\n","Epoch 15: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 8.0311e-06 - accuracy: 1.0000 - val_loss: 7.0833e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 16/30\n","312/312 [==============================] - ETA: 0s - loss: 7.9829e-06 - accuracy: 1.0000\n","Epoch 16: BER = 0.000000000000000\n","Epoch 16: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 7.9829e-06 - accuracy: 1.0000 - val_loss: 6.8970e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 17/30\n","311/312 [============================>.] - ETA: 0s - loss: 8.0244e-06 - accuracy: 1.0000\n","Epoch 17: BER = 0.000000000000000\n","Epoch 17: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 8.0201e-06 - accuracy: 1.0000 - val_loss: 6.6976e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 18/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.6142e-06 - accuracy: 1.0000\n","Epoch 18: BER = 0.000000000000000\n","Epoch 18: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 7.6050e-06 - accuracy: 1.0000 - val_loss: 6.5124e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 19/30\n","312/312 [==============================] - ETA: 0s - loss: 7.3801e-06 - accuracy: 1.0000\n","Epoch 19: BER = 0.000000000000000\n","Epoch 19: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 18ms/step - loss: 7.3801e-06 - accuracy: 1.0000 - val_loss: 6.3374e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n","Epoch 20/30\n","309/312 [============================>.] - ETA: 0s - loss: 7.2912e-06 - accuracy: 1.0000\n","Epoch 20: BER = 0.000000000000000\n","Epoch 20: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.2837e-06 - accuracy: 1.0000 - val_loss: 6.2780e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 21/30\n","311/312 [============================>.] - ETA: 0s - loss: 7.1168e-06 - accuracy: 1.0000\n","Epoch 21: BER = 0.000000000000000\n","Epoch 21: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 7.1158e-06 - accuracy: 1.0000 - val_loss: 6.1950e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 22/30\n","311/312 [============================>.] - ETA: 0s - loss: 7.1111e-06 - accuracy: 1.0000\n","Epoch 22: BER = 0.000000000000000\n","Epoch 22: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 7.1082e-06 - accuracy: 1.0000 - val_loss: 6.1310e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 23/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.9862e-06 - accuracy: 1.0000\n","Epoch 23: BER = 0.000000000000000\n","Epoch 23: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 15ms/step - loss: 6.9833e-06 - accuracy: 1.0000 - val_loss: 6.0252e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 24/30\n","311/312 [============================>.] - ETA: 0s - loss: 6.9403e-06 - accuracy: 1.0000\n","Epoch 24: BER = 0.000000000000000\n","Epoch 24: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.9395e-06 - accuracy: 1.0000 - val_loss: 5.9361e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 25/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.7243e-06 - accuracy: 1.0000\n","Epoch 25: BER = 0.000000000000000\n","Epoch 25: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 6.7161e-06 - accuracy: 1.0000 - val_loss: 5.8373e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 26/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.7436e-06 - accuracy: 1.0000\n","Epoch 26: BER = 0.000000000000000\n","Epoch 26: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 6s 19ms/step - loss: 6.7339e-06 - accuracy: 1.0000 - val_loss: 5.7192e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 27/30\n","312/312 [==============================] - ETA: 0s - loss: 6.6722e-06 - accuracy: 1.0000\n","Epoch 27: BER = 0.000000000000000\n","Epoch 27: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.6722e-06 - accuracy: 1.0000 - val_loss: 5.5988e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 28/30\n","309/312 [============================>.] - ETA: 0s - loss: 6.4107e-06 - accuracy: 1.0000\n","Epoch 28: BER = 0.000000000000000\n","Epoch 28: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 17ms/step - loss: 6.4058e-06 - accuracy: 1.0000 - val_loss: 5.4083e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 29/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.1921e-06 - accuracy: 1.0000\n","Epoch 29: BER = 0.000000000000000\n","Epoch 29: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 4s 13ms/step - loss: 6.1907e-06 - accuracy: 1.0000 - val_loss: 5.2407e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","Epoch 30/30\n","310/312 [============================>.] - ETA: 0s - loss: 6.0102e-06 - accuracy: 1.0000\n","Epoch 30: BER = 0.000000000000000\n","Epoch 30: Accuracy = 1.000000000000000, Validation Accuracy = 1.000000000000000\n","312/312 [==============================] - 5s 16ms/step - loss: 6.0052e-06 - accuracy: 1.0000 - val_loss: 5.1074e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n","78/78 [==============================] - 1s 6ms/step - loss: 5.1075e-06 - accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["plt.figure\n","plt.semilogy(SNR,Loss)\n","plt.xlabel(\"SNR\")\n","plt.ylabel(\"validation Loss\")\n","plt.show()"],"metadata":{"id":"VPkoS0HLsB8R","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"ok","timestamp":1718292344469,"user_tz":-120,"elapsed":828,"user":{"displayName":"Wei CHEN","userId":"09604576644756669971"}},"outputId":"01cb2ea3-c235-4fab-c834-1e51c625baa0"},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["SNR = np.linspace(0,30,10)\n","Z = np.linspace(1,9,9)\n","# Z = 5\n","P = 0.01\n","clearwater = Watertype(0.551,0)\n","# Loss = []\n","# Accuracy = []\n","X, Y = np.meshgrid(SNR, Z)\n","# Create 3D array\n","Loss = np.zeros_like(X)\n","Accuracy = np.zeros_like(X)\n","for j in range(len(Z)):\n","  for  i in range(len(SNR)):\n","    S_1,S_0,Denm_trm1,Denm_trm0,Threshold_optimal = Simulated_dataset(SNR[i],Z[j],P,clearwater,Noise='Gauss')\n","    Label0 = np.zeros(len(S_0))\n","    Label1 = np.ones(len(S_1))\n","    # File path\n","    file_path = 'supervised_dataset.csv'\n","    # Check if the file exists and delete it if it does\n","    if os.path.exists(file_path):\n","        os.remove(file_path)\n","    # Create a DataFrame\n","    data1 = pd.DataFrame({'Feature': S_1, 'Label': Label1})\n","    data0 = pd.DataFrame({'Feature': S_0, 'Label': Label0})\n","    # combine the bit 1 and bit 0 signal\n","    merged_data = pd.concat([data1, data0], ignore_index=True)\n","    # Save the dataset to a CSV file\n","    merged_data.to_csv(file_path, index=False)\n","\n","    data = pd.read_csv(file_path)\n","    shuffled_data = shuffle(data)\n","    # Split the dataset into features (X) and labels (y)\n","    X = shuffled_data[['Feature']].values\n","    Y = shuffled_data['Label'].values\n","    # Split the data into training and testing sets\n","    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","    # Create data generators\n","    batch_size = 32\n","    training_generator = DataGenerator(X_train, Y_train, batch_size=batch_size)\n","    validation_generator = DataGenerator(X_test, Y_test, batch_size=batch_size)\n","    # Adjust input_length to match the number of features\n","    input_length = X_train.shape[1]\n","\n","    # Create the model\n","    CNN0 = create_model(input_length)\n","    # Learning rate scheduler\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n","    # Train the model using the data generator\n","    # history = CNN0.fit(training_generator, validation_data=validation_generator, epochs=30)\n","    history = CNN0.fit(training_generator, validation_data=validation_generator, epochs=30, callbacks=[reduce_lr, CustomCallback()])\n","    # Evaluate the model\n","    loss, accuracy = CNN0.evaluate(validation_generator)\n","    Loss[j,i] = loss\n","    # Loss.append(loss)\n","    Accuracy[j,i] = accuracy\n"],"metadata":{"id":"maOVgkyVqWAg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","# Plot the surface\n","surf = ax.plot_surface(X, Y,Loss,cmap='viridis')\n","# Set labels\n","ax.set_xlabel('SNR')\n","ax.set_ylabel('Z')\n","ax.set_zlabel('Loss')"],"metadata":{"id":"SMS7Gu1gradM"},"execution_count":null,"outputs":[]}]}